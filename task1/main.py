import itertools

from numpy import iterable
from app.config import DATA_JSON, CHUNK_MIN, CHUNK_MAX, CHUNK_OVERLAP
from app.parser import parse_cases
from app.chunker import build_chunk_entries
from app.embedding_client import EmbeddingClient
from app.db_connection import VectorDB


def batched(iterable, n):
    it = iter(iterable)
    while True:
        batch = list(itertools.islice(it, n))
        if not batch:
            break
        yield batch


def run_task1():
    """ì„¹ì…˜ë³„ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì•ˆì •ì„± í–¥ìƒ"""
    print("ğŸš€ Task1 ì‹œì‘: ì¼€ì´ìŠ¤ë³„ ì„¹ì…˜ ì²˜ë¦¬")
    
    records = parse_cases(DATA_JSON)
    print(f"ğŸ“š ì´ {len(records)}ê°œ ì¼€ì´ìŠ¤ íŒŒì‹± ì™„ë£Œ")
    
    # ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    embedder = EmbeddingClient()
    
    # VectorDB ì´ˆê¸°í™” (ì²« ë²ˆì§¸ ì¼€ì´ìŠ¤ì—ì„œ ì°¨ì› ì„¤ì •)
    vdb = None
    total_processed = 0
    
    for case_idx, rec in enumerate(records):
        print(f"\nğŸ“‹ ì¼€ì´ìŠ¤ {case_idx + 1}/{len(records)} ì²˜ë¦¬ ì¤‘: {rec.get('title', 'Unknown')}")
        
        # í˜„ì¬ ì¼€ì´ìŠ¤ì˜ ì²­í¬ ìƒì„±
        case_entries = build_chunk_entries(rec, CHUNK_MIN, CHUNK_MAX, CHUNK_OVERLAP)
        print(f"   ğŸ“ ì²­í¬ ìˆ˜: {len(case_entries)}")
        
        if not case_entries:
            print("   âš ï¸  ë¹ˆ ì¼€ì´ìŠ¤, ê±´ë„ˆëœ€")
            continue
        
        # ì¼€ì´ìŠ¤ë³„ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        texts = [e["text"] for e in case_entries]
        print(f"   ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        try:
            emb_out = embedder.embed(texts, batch_size=8)  # ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì•ˆì •ì„± í–¥ìƒ
            vectors, dim = emb_out["embeddings"], emb_out["dim"]
            print(f"   âœ… ì„ë² ë”© ì™„ë£Œ: {vectors.shape}")
            
            # VectorDB ì´ˆê¸°í™” (ì²« ë²ˆì§¸ ì„±ê³µí•œ ì¼€ì´ìŠ¤ì—ì„œ)
            if vdb is None:
                print(f"   ğŸ”§ VectorDB ì´ˆê¸°í™” (ì°¨ì›: {dim})")
                vdb = VectorDB(dim=dim)
            
            # íŒë¡€ì •ë³´ì¼ë ¨ë²ˆí˜¸ë¥¼ IDë¡œ ì‚¬ìš© (ASCII í˜¸í™˜)
            ids = []
            for e in case_entries:
                # íŒë¡€ì •ë³´ì¼ë ¨ë²ˆí˜¸ + ì²­í¬ì¸ë±ìŠ¤ë¡œ ê³ ìœ  ID ìƒì„± (íŒê²°ìš”ì§€ë§Œ ì²˜ë¦¬í•˜ë¯€ë¡œ ê°„ë‹¨)
                case_id = e.get('source_id', 'unknown')  # íŒë¡€ì •ë³´ì¼ë ¨ë²ˆí˜¸
                chunk_idx = e.get('chunk_idx', 0)
                unique_id = f"{case_id}_summary_{chunk_idx}"  # summaryë¡œ ê³ ì •
                ids.append(unique_id)
            
            metas = [e["meta"] for e in case_entries]
            
            # ë°°ì¹˜ë³„ë¡œ Pineconeì— ì—…ë¡œë“œ
            batch_size = 64  # Pinecone ë°°ì¹˜ í¬ê¸°
            for i in range(0, len(ids), batch_size):
                batch_ids = ids[i:i+batch_size]
                batch_vecs = [vectors[j].tolist() for j in range(i, min(i+batch_size, len(vectors)))]
                batch_metas = metas[i:i+batch_size]
                
                print(f"   ğŸ“¤ ë°°ì¹˜ ì—…ë¡œë“œ: {len(batch_ids)}ê°œ ë²¡í„°")
                vdb.upsert(batch_ids, batch_vecs, batch_metas)
                print(f"   âœ… ì—…ë¡œë“œ ì™„ë£Œ: {batch_ids[0]} ... {batch_ids[-1]}")
            
            total_processed += len(case_entries)
            print(f"   ğŸ¯ ì¼€ì´ìŠ¤ ì™„ë£Œ. ëˆ„ì  ì²˜ë¦¬: {total_processed}ê°œ ì²­í¬")
            
        except Exception as e:
            print(f"   âŒ ì¼€ì´ìŠ¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            print(f"   â­ï¸  ë‹¤ìŒ ì¼€ì´ìŠ¤ë¡œ ê³„ì†...")
            continue
    
    print(f"\nğŸ‰ Task1 ì™„ë£Œ!")
    print(f"ğŸ“Š ìµœì¢… í†µê³„:")
    print(f"   - ì²˜ë¦¬ëœ ì¼€ì´ìŠ¤: {len(records)}ê°œ")
    print(f"   - ì´ ì²­í¬ ìˆ˜: {total_processed}ê°œ")
    print(f"   - VectorDB ìƒíƒœ: {'ì´ˆê¸°í™” ì™„ë£Œ' if vdb else 'ì´ˆê¸°í™” ì‹¤íŒ¨'}")


if __name__ == "__main__":
    run_task1()
import os, time
import numpy as np
from typing import Any, Dict, List, Sequence
from dotenv import load_dotenv
from google.genai import Client
from tqdm import tqdm



class EmbeddingClient:
    def __init__(self, model="gemini-embedding-001", api_key_env="GOOGLE_API_KEY", normalize=True):
        # Load environment variables from the correct path
        env_path = os.path.join(os.path.dirname(__file__), '..', '..', 'env', '.env')
        load_dotenv(env_path)
        api_key = os.getenv(api_key_env)
        if not api_key:
            raise RuntimeError(f"í™˜ê²½ë³€ìˆ˜ {api_key_env}ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤")
        self.client = Client(api_key=api_key)
        self.model = model
        self.normalize = normalize


    def _embed_once(self, texts: Sequence[str]) -> List[List[float]]:
        vecs = []
        print(f"ğŸ”„ ì„ë² ë”© ì²˜ë¦¬ ì¤‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
        
        for i, t in enumerate(tqdm(texts, desc="ì„ë² ë”© ìƒì„±", unit="text")):
            max_retries = 3
            retry_delay = 2.0
            
            for attempt in range(max_retries):
                try:
                    print(f"ğŸ“ ì²˜ë¦¬ ì¤‘: {i+1}/{len(texts)} - ì‹œë„ {attempt+1}")
                    resp = self.client.models.embed_content(model=self.model, contents=t)
                    vecs.append(resp.embeddings[0].values)
                    print(f"âœ… ì„±ê³µ: {i+1}/{len(texts)}")
                    # Add delay to avoid rate limiting
                    time.sleep(1.5)  # Increased delay to 1.5 seconds
                    break
                except Exception as e:
                    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt+1}/{max_retries}): {e}")
                    if "429" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                        print(f"â³ í• ë‹¹ëŸ‰ ì´ˆê³¼. {retry_delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                        time.sleep(retry_delay)
                        retry_delay *= 2  # Exponential backoff
                    elif attempt == max_retries - 1:
                        print(f"ğŸ’€ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ì˜ë²¡í„°ë¡œ ëŒ€ì²´")
                        # Return zero vector on error
                        vecs.append([0.0] * 768)  # Assuming 768-dimensional embeddings
                    else:
                        time.sleep(retry_delay)
        
        print(f"ğŸ‰ ì„ë² ë”© ì™„ë£Œ: {len(vecs)}ê°œ ë²¡í„° ìƒì„±")
        return vecs


    def embed(self, texts: Sequence[str], batch_size=16) -> Dict[str, Any]:
        print(f"ğŸš€ ì„ë² ë”© ì‹œì‘: ì´ {len(texts)}ê°œ í…ìŠ¤íŠ¸, ë°°ì¹˜ í¬ê¸°: {batch_size}")
        all_vecs = []
        
        total_batches = (len(texts) + batch_size - 1) // batch_size
        
        for i in tqdm(range(0, len(texts), batch_size), desc="ë°°ì¹˜ ì²˜ë¦¬", total=total_batches):
            batch_num = i // batch_size + 1
            chunk = texts[i:i+batch_size]
            print(f"\nğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘ ({len(chunk)}ê°œ í…ìŠ¤íŠ¸)")
            
            batch_vecs = self._embed_once(chunk)
            all_vecs.extend(batch_vecs)
            
            print(f"âœ… ë°°ì¹˜ {batch_num} ì™„ë£Œ. ëˆ„ì : {len(all_vecs)}ê°œ ë²¡í„°")
            
            # Add delay between batches to avoid rate limiting
            if i + batch_size < len(texts):
                print("â³ ë‹¤ìŒ ë°°ì¹˜ ì „ 3ì´ˆ ëŒ€ê¸°...")
                time.sleep(3.0)
        
        arr = np.array(all_vecs, dtype=np.float32)
        if self.normalize:
            print("ğŸ”§ ë²¡í„° ì •ê·œí™” ì¤‘...")
            arr = self._l2_normalize(arr)
            print("âœ… ì •ê·œí™” ì™„ë£Œ")
        
        print(f"ğŸ¯ ìµœì¢… ê²°ê³¼: {arr.shape[0]}ê°œ ë²¡í„°, ì°¨ì›: {arr.shape[1]}")
        return {"embeddings": arr, "dim": arr.shape[1]}


    def embed_query(self, text: str):
        print(f"ğŸ” ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì¤‘...")
        result = self.embed([text])["embeddings"][0]
        print(f"âœ… ì¿¼ë¦¬ ì„ë² ë”© ì™„ë£Œ")
        return result


    @staticmethod
    def _l2_normalize(x: np.ndarray, eps=1e-12):
        norm = np.linalg.norm(x, axis=1, keepdims=True)
        return x / np.maximum(norm, eps)
#!/usr/bin/env python3
"""
ë³´ê³ ì„œ ìƒì„±ê¸° ëª¨ë“ˆ

ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìƒì„¸í•œ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import os
import json
from datetime import datetime
from typing import Dict, List
from pathlib import Path


class ReportGenerator:
    """ë³´ê³ ì„œ ìƒì„± í´ë˜ìŠ¤"""
    
    def __init__(self, performance_results: Dict, quality_results: Dict, analysis_start_time: datetime):
        """ë³´ê³ ì„œ ìƒì„±ê¸° ì´ˆê¸°í™”"""
        self.performance_results = performance_results
        self.quality_results = quality_results
        self.analysis_start_time = analysis_start_time
        self.report_dir = Path(__file__).parent.parent / "reports"
        self.report_dir.mkdir(exist_ok=True)
    
    def generate_markdown_report(self) -> str:
        """Markdown í˜•ì‹ ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        
        report_content = f"""# ë²•ë¥  AI ì‹œìŠ¤í…œ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ

## ğŸ“‹ ë¶„ì„ ê°œìš”

**ë¶„ì„ ì¼ì‹œ**: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}  
**ë¶„ì„ ì†Œìš”ì‹œê°„**: {(datetime.now() - self.analysis_start_time).total_seconds():.1f}ì´ˆ  
**ë³´ê³ ì„œ ë²„ì „**: 1.0  

---

## ğŸ¯ Executive Summary

### ì£¼ìš” ì„±ê³¼
{self._generate_key_achievements()}

### í•µì‹¬ ì§€í‘œ
{self._generate_key_metrics()}

### ê¶Œì¥ì‚¬í•­
{self._generate_recommendations()}

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë¶„ì„

### Task 1: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
- **ë°ì´í„° ì²˜ë¦¬**: í•œêµ­ ë²•ë¥  íŒë¡€ ë°ì´í„° íŒŒì‹± ë° ì²­í‚¹
- **ì„ë² ë”© ëª¨ë¸**: Google Gemini Embedding (3072ì°¨ì›)
- **ë²¡í„° ì €ì¥ì†Œ**: Pinecone í´ë¼ìš°ë“œ ê¸°ë°˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
- **ì²­í‚¹ ì „ëµ**: íŒê²°ìš”ì§€ ì¤‘ì‹¬ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ (300ì ê¸°ì¤€)

### Task 2: RAG ê¸°ë°˜ ì±—ë´‡
- **ê²€ìƒ‰ ì‹œìŠ¤í…œ**: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰
- **ìƒì„± ëª¨ë¸**: Google Gemini 2.0 Flash
- **ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤**: Gradio ì›¹ ê¸°ë°˜ ì¸í„°í˜ì´ìŠ¤
- **ëŒ€í™” ê´€ë¦¬**: ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ ë° íˆìŠ¤í† ë¦¬ ê´€ë¦¬

---

## ğŸ“Š ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼

### ì„ë² ë”© ì„±ëŠ¥
{self._generate_embedding_performance()}

### ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥
{self._generate_vector_db_performance()}

### ê²€ìƒ‰ ì •í™•ë„
{self._generate_retrieval_accuracy()}

---

## ğŸ¯ í’ˆì§ˆ í‰ê°€ ê²°ê³¼

### ì±—ë´‡ ì‘ë‹µ í’ˆì§ˆ
{self._generate_chatbot_quality()}

### ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥
{self._generate_category_performance()}

### ì‘ë‹µ íŒ¨í„´ ë¶„ì„
{self._generate_response_patterns()}

---

## ğŸ” ìƒì„¸ ë¶„ì„

### ì‹œìŠ¤í…œ ê°•ì 
{self._generate_strengths()}

### ê°œì„  ì˜ì—­
{self._generate_weaknesses()}

### ê¸°ìˆ ì  ê³ ë ¤ì‚¬í•­
{self._generate_technical_considerations()}

---

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­

### ë‹¨ê¸° ê°œì„  ë°©ì•ˆ
{self._generate_short_term_improvements()}

### ì¤‘ì¥ê¸° ë¡œë“œë§µ
{self._generate_long_term_roadmap()}

---

## ğŸ”¬ ì‹¤í—˜ ê²°ê³¼ ìƒì„¸

### í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¶„ì„
{self._generate_test_case_analysis()}

### ì˜¤ë¥˜ ë¶„ì„
{self._generate_error_analysis()}

---

## ğŸ“ ê²°ë¡ 

{self._generate_conclusion()}

---

## ğŸ“š ë¶€ë¡

### A. ê¸°ìˆ  ìŠ¤íƒ
- **Backend**: Python 3.x
- **ì„ë² ë”©**: Google GenAI Embedding
- **ë²¡í„° DB**: Pinecone
- **LLM**: Google Gemini 2.0 Flash
- **Frontend**: Gradio
- **ë°ì´í„°**: í•œêµ­ ë²•ë¥  íŒë¡€ ë°ì´í„°

### B. í™˜ê²½ ì„¤ì •
```bash
# í•„ìš” íŒ¨í‚¤ì§€
pip install google-genai pinecone gradio python-dotenv numpy pandas

# í™˜ê²½ ë³€ìˆ˜
GOOGLE_API_KEY=your_google_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=law-bot-korean
```

### C. ì°¸ê³  ìë£Œ
- Task 1 êµ¬í˜„: `bhsn/task1/`
- Task 2 êµ¬í˜„: `bhsn/task2/`
- ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸: `bhsn/task3/analysis/`

---

*ë³¸ ë³´ê³ ì„œëŠ” ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì •í™•í•œ ë²•ë¥  ìë¬¸ì€ ì „ë¬¸ ë³€í˜¸ì‚¬ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.*
"""
        
        return report_content
    
    def _generate_key_achievements(self) -> str:
        """ì£¼ìš” ì„±ê³¼ ì„¹ì…˜ ìƒì„±"""
        achievements = [
            "âœ… **ë²•ë¥  ë°ì´í„° ë²¡í„°í™” ì™„ë£Œ**: í•œêµ­ ë²•ë¥  íŒë¡€ ë°ì´í„°ì˜ íš¨ê³¼ì ì¸ ì„ë² ë”© ë° ì¸ë±ì‹±",
            "âœ… **RAG ê¸°ë°˜ ì±—ë´‡ êµ¬í˜„**: ê²€ìƒ‰ ì¦ê°• ìƒì„±ì„ í†µí•œ ë²•ë¥  ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ êµ¬ì¶•",
            "âœ… **ì›¹ ì¸í„°í˜ì´ìŠ¤ ì œê³µ**: ì‚¬ìš©ì ì¹œí™”ì ì¸ Gradio ê¸°ë°˜ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤"
        ]
        
        # ì„±ëŠ¥ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì¶”ê°€
        if self.performance_results and 'total_errors' in self.performance_results:
            if self.performance_results['total_errors'] == 0:
                achievements.append("âœ… **ì•ˆì •ì  ì‹œìŠ¤í…œ ìš´ì˜**: ë¶„ì„ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ì—†ì´ ë™ì‘")
        
        if self.quality_results and 'test_results' in self.quality_results:
            test_results = self.quality_results['test_results']
            if test_results.get('successful_responses', 0) > 0:
                success_rate = test_results['successful_responses'] / test_results['total_questions']
                achievements.append(f"âœ… **ë†’ì€ ì‘ë‹µ ì„±ê³µë¥ **: {success_rate:.1%}ì˜ ì§ˆë¬¸ ì‘ë‹µ ì„±ê³µ")
        
        return '\n'.join(achievements)
    
    def _generate_key_metrics(self) -> str:
        """í•µì‹¬ ì§€í‘œ ì„¹ì…˜ ìƒì„±"""
        metrics = []
        
        # ì„±ëŠ¥ ì§€í‘œ
        if self.performance_results:
            if 'embedding_performance' in self.performance_results:
                embed_perf = self.performance_results['embedding_performance']
                if 'average_time' in embed_perf and embed_perf['average_time'] > 0:
                    metrics.append(f"ğŸ• **ì„ë² ë”© ì†ë„**: {embed_perf['average_time']:.3f}ì´ˆ/í…ìŠ¤íŠ¸")
                if 'dimension' in embed_perf:
                    metrics.append(f"ğŸ“ **ì„ë² ë”© ì°¨ì›**: {embed_perf['dimension']}ì°¨ì›")
            
            if 'vector_db_performance' in self.performance_results:
                db_perf = self.performance_results['vector_db_performance']
                if 'search_performance' in db_perf and 'average_time' in db_perf['search_performance']:
                    avg_time = db_perf['search_performance']['average_time']
                    metrics.append(f"ğŸ” **ê²€ìƒ‰ ì†ë„**: {avg_time:.3f}ì´ˆ/ì¿¼ë¦¬")
        
        # í’ˆì§ˆ ì§€í‘œ
        if self.quality_results and 'test_results' in self.quality_results:
            test_results = self.quality_results['test_results']
            if 'average_scores' in test_results:
                avg_scores = test_results['average_scores']
                metrics.append(f"â­ **í‰ê·  ê´€ë ¨ì„±**: {avg_scores.get('relevance', 0):.1f}/10")
                metrics.append(f"ğŸ¯ **í‰ê·  ì •í™•ë„**: {avg_scores.get('accuracy', 0):.1f}/10")
        
        if not metrics:
            metrics.append("ğŸ“Š ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘ ì¤‘...")
        
        return '\n'.join(metrics)
    
    def _generate_recommendations(self) -> str:
        """ê¶Œì¥ì‚¬í•­ ì„¹ì…˜ ìƒì„±"""
        recommendations = []
        
        # í’ˆì§ˆ í‰ê°€ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if (self.quality_results and 'overall_assessment' in self.quality_results and 
            'recommendations' in self.quality_results['overall_assessment']):
            for rec in self.quality_results['overall_assessment']['recommendations']:
                recommendations.append(f"ğŸ’¡ {rec}")
        
        # ì„±ëŠ¥ ë¶„ì„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if self.performance_results:
            if self.performance_results.get('total_errors', 0) > 0:
                recommendations.append("âš ï¸ ì‹œìŠ¤í…œ ì•ˆì •ì„± ê°œì„ ì„ ìœ„í•œ ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™” í•„ìš”")
        
        # ê¸°ë³¸ ê¶Œì¥ì‚¬í•­
        if not recommendations:
            recommendations = [
                "ğŸ’¡ ë²¡í„° ê²€ìƒ‰ ìµœì í™”ë¥¼ í†µí•œ ì‘ë‹µ ì†ë„ ê°œì„ ",
                "ğŸ’¡ ë” ë§ì€ ë²•ë¥  ë„ë©”ì¸ ë°ì´í„° ì¶”ê°€",
                "ğŸ’¡ ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ë° ë°˜ì˜ ì²´ê³„ êµ¬ì¶•"
            ]
        
        return '\n'.join(recommendations)
    
    def _generate_embedding_performance(self) -> str:
        """ì„ë² ë”© ì„±ëŠ¥ ì„¹ì…˜ ìƒì„±"""
        if not self.performance_results or 'embedding_performance' not in self.performance_results:
            return "âš ï¸ ì„ë² ë”© ì„±ëŠ¥ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        embed_perf = self.performance_results['embedding_performance']
        content = []
        
        if 'average_time' in embed_perf:
            content.append(f"- **í‰ê·  ì²˜ë¦¬ ì‹œê°„**: {embed_perf['average_time']:.3f}ì´ˆ")
        
        if 'throughput' in embed_perf:
            content.append(f"- **ì²˜ë¦¬ëŸ‰**: {embed_perf['throughput']:.1f} texts/second")
        
        if 'dimension' in embed_perf:
            content.append(f"- **ë²¡í„° ì°¨ì›**: {embed_perf['dimension']}ì°¨ì›")
        
        if 'batch_performance' in embed_perf:
            content.append("- **ë°°ì¹˜ ì„±ëŠ¥**:")
            for batch_size, perf in embed_perf['batch_performance'].items():
                time_per_item = perf.get('time_per_item', 0)
                content.append(f"  - ë°°ì¹˜ í¬ê¸° {batch_size}: {time_per_item:.3f}ì´ˆ/item")
        
        if embed_perf.get('errors', 0) > 0:
            content.append(f"- âš ï¸ **ì˜¤ë¥˜ ë°œìƒ**: {embed_perf['errors']}ê±´")
        
        return '\n'.join(content) if content else "ë°ì´í„° ì—†ìŒ"
    
    def _generate_vector_db_performance(self) -> str:
        """ë²¡í„° DB ì„±ëŠ¥ ì„¹ì…˜ ìƒì„±"""
        if not self.performance_results or 'vector_db_performance' not in self.performance_results:
            return "âš ï¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        db_perf = self.performance_results['vector_db_performance']
        content = []
        
        if 'search_performance' in db_perf:
            search_perf = db_perf['search_performance']
            if 'average_time' in search_perf:
                content.append(f"- **í‰ê·  ê²€ìƒ‰ ì‹œê°„**: {search_perf['average_time']:.3f}ì´ˆ")
                content.append(f"- **ê²€ìƒ‰ ì²˜ë¦¬ëŸ‰**: {1.0/search_perf['average_time']:.1f} queries/second")
            
            if 'min_time' in search_perf and 'max_time' in search_perf:
                content.append(f"- **ê²€ìƒ‰ ì‹œê°„ ë²”ìœ„**: {search_perf['min_time']:.3f}ì´ˆ ~ {search_perf['max_time']:.3f}ì´ˆ")
        
        if 'index_stats' in db_perf and db_perf['index_stats']:
            content.append("- **ì¸ë±ìŠ¤ ì •ë³´**: ì •ìƒ ì—°ê²°")
        
        if db_perf.get('errors', 0) > 0:
            content.append(f"- âš ï¸ **ì˜¤ë¥˜ ë°œìƒ**: {db_perf['errors']}ê±´")
        
        return '\n'.join(content) if content else "ë°ì´í„° ì—†ìŒ"
    
    def _generate_retrieval_accuracy(self) -> str:
        """ê²€ìƒ‰ ì •í™•ë„ ì„¹ì…˜ ìƒì„±"""
        if not self.performance_results or 'retrieval_accuracy' not in self.performance_results:
            return "âš ï¸ ê²€ìƒ‰ ì •í™•ë„ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        accuracy = self.performance_results['retrieval_accuracy']
        content = []
        
        if 'precision_at_k' in accuracy:
            content.append("**Precision@K ê²°ê³¼**:")
            for metric, data in accuracy['precision_at_k'].items():
                if 'mean' in data:
                    content.append(f"- {metric.upper()}: {data['mean']:.3f}")
        
        if accuracy.get('errors', 0) > 0:
            content.append(f"- âš ï¸ **ì˜¤ë¥˜ ë°œìƒ**: {accuracy['errors']}ê±´")
        
        return '\n'.join(content) if content else "ë°ì´í„° ì—†ìŒ"
    
    def _generate_chatbot_quality(self) -> str:
        """ì±—ë´‡ í’ˆì§ˆ ì„¹ì…˜ ìƒì„±"""
        if not self.quality_results or 'test_results' not in self.quality_results:
            return "âš ï¸ ì±—ë´‡ í’ˆì§ˆ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        test_results = self.quality_results['test_results']
        content = []
        
        # ê¸°ë³¸ í†µê³„
        total = test_results.get('total_questions', 0)
        success = test_results.get('successful_responses', 0)
        failed = test_results.get('failed_responses', 0)
        
        content.append(f"- **ì´ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸**: {total}ê°œ")
        success_rate = success/total if total > 0 else 0
        content.append(f"- **ì„±ê³µë¥ **: {success}/{total} ({success_rate:.1%})")
        
        # í‰ê·  ì ìˆ˜
        if 'average_scores' in test_results:
            avg_scores = test_results['average_scores']
            content.append("- **í‰ê·  ì ìˆ˜**:")
            content.append(f"  - ê´€ë ¨ì„±: {avg_scores.get('relevance', 0):.1f}/10")
            content.append(f"  - ì™„ì„±ë„: {avg_scores.get('completeness', 0):.1f}/10")
            content.append(f"  - ì •í™•ë„: {avg_scores.get('accuracy', 0):.1f}/10")
            content.append(f"  - ëª…í™•ì„±: {avg_scores.get('clarity', 0):.1f}/10")
            content.append(f"  - ê°œë… í¬í•¨ë„: {avg_scores.get('concept_coverage', 0):.1%}")
        
        return '\n'.join(content)
    
    def _generate_category_performance(self) -> str:
        """ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ì„¹ì…˜ ìƒì„±"""
        if not self.quality_results or 'test_results' not in self.quality_results:
            return "âš ï¸ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        test_results = self.quality_results['test_results']
        
        if 'category_analysis' not in test_results:
            return "ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ë°ì´í„° ì—†ìŒ"
        
        content = []
        for category, stats in test_results['category_analysis'].items():
            content.append(f"**{category}**")
            content.append(f"- ì§ˆë¬¸ ìˆ˜: {stats.get('question_count', 0)}ê°œ")
            content.append(f"- í‰ê·  ê´€ë ¨ì„±: {stats.get('avg_relevance', 0):.1f}/10")
            content.append(f"- í‰ê·  ì •í™•ë„: {stats.get('avg_accuracy', 0):.1f}/10")
            content.append(f"- ê°œë… í¬í•¨ë„: {stats.get('avg_concept_coverage', 0):.1%}")
            content.append("")
        
        return '\n'.join(content)
    
    def _generate_response_patterns(self) -> str:
        """ì‘ë‹µ íŒ¨í„´ ì„¹ì…˜ ìƒì„±"""
        if not self.quality_results or 'pattern_analysis' not in self.quality_results:
            return "âš ï¸ ì‘ë‹µ íŒ¨í„´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        pattern = self.quality_results['pattern_analysis']
        content = []
        
        # ë²•ì  ê·¼ê±° í¬í•¨ë¥ 
        if 'legal_reference_rate' in pattern:
            rate = pattern['legal_reference_rate']
            content.append(f"- **ë²•ì  ê·¼ê±° í¬í•¨ë¥ **: {rate:.1%}")
        
        # ì‘ë‹µ ê¸¸ì´ í†µê³„
        if 'response_length_stats' in pattern:
            length_stats = pattern['response_length_stats']
            content.append("- **ì‘ë‹µ ê¸¸ì´ í†µê³„**:")
            content.append(f"  - í‰ê· : {length_stats.get('average', 0):.0f}ì")
            content.append(f"  - ë²”ìœ„: {length_stats.get('min', 0)}ì ~ {length_stats.get('max', 0)}ì")
        
        # ê³µí†µ ë¬¸ì œì 
        if 'common_issues' in pattern and pattern['common_issues']:
            content.append("- **ì£¼ìš” ë¬¸ì œì **:")
            for issue, count in sorted(pattern['common_issues'].items(), 
                                     key=lambda x: x[1], reverse=True):
                content.append(f"  - {issue}: {count}íšŒ")
        
        return '\n'.join(content)
    
    def _generate_strengths(self) -> str:
        """ê°•ì  ì„¹ì…˜ ìƒì„±"""
        strengths = []
        
        if (self.quality_results and 'overall_assessment' in self.quality_results and 
            'strengths' in self.quality_results['overall_assessment']):
            for strength in self.quality_results['overall_assessment']['strengths']:
                strengths.append(f"âœ… {strength}")
        
        # ê¸°ë³¸ ê°•ì ë“¤
        default_strengths = [
            "âœ… í•œêµ­ì–´ ë²•ë¥  ë„ë©”ì¸ íŠ¹í™” ì‹œìŠ¤í…œ",
            "âœ… ìµœì‹  ì„ë² ë”© ë° ìƒì„± ëª¨ë¸ í™œìš©",
            "âœ… í™•ì¥ ê°€ëŠ¥í•œ í´ë¼ìš°ë“œ ê¸°ë°˜ ì•„í‚¤í…ì²˜"
        ]
        
        if not strengths:
            strengths = default_strengths
        
        return '\n'.join(strengths)
    
    def _generate_weaknesses(self) -> str:
        """ì•½ì  ì„¹ì…˜ ìƒì„±"""
        weaknesses = []
        
        if (self.quality_results and 'overall_assessment' in self.quality_results and 
            'weaknesses' in self.quality_results['overall_assessment']):
            for weakness in self.quality_results['overall_assessment']['weaknesses']:
                weaknesses.append(f"âš ï¸ {weakness}")
        
        # ì‹œìŠ¤í…œ ì˜¤ë¥˜ ê¸°ë°˜ ì•½ì 
        if self.performance_results and self.performance_results.get('total_errors', 0) > 0:
            weaknesses.append("âš ï¸ ì‹œìŠ¤í…œ ì•ˆì •ì„± ê°œì„  í•„ìš”")
        
        if not weaknesses:
            weaknesses.append("ğŸ” ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•œ ì˜ì—­ ì—†ìŒ")
        
        return '\n'.join(weaknesses)
    
    def _generate_technical_considerations(self) -> str:
        """ê¸°ìˆ ì  ê³ ë ¤ì‚¬í•­ ì„¹ì…˜ ìƒì„±"""
        considerations = [
            "ğŸ”§ **í™•ì¥ì„±**: ë” ë§ì€ ë²•ë¥  ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‹œìŠ¤í…œ í™•ì¥ ê³ ë ¤",
            "ğŸ”’ **ë³´ì•ˆ**: ë¯¼ê°í•œ ë²•ë¥  ì •ë³´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë³´ì•ˆ ê°•í™”",
            "âš¡ **ì„±ëŠ¥**: ì‹¤ì‹œê°„ ì‘ë‹µì„ ìœ„í•œ ê²€ìƒ‰ ë° ìƒì„± ì†ë„ ìµœì í™”",
            "ğŸ”„ **ì—…ë°ì´íŠ¸**: ë²•ë¥  ë³€ê²½ì‚¬í•­ ë°˜ì˜ì„ ìœ„í•œ ì£¼ê¸°ì  ë°ì´í„° ì—…ë°ì´íŠ¸",
            "ğŸ“Š **ëª¨ë‹ˆí„°ë§**: ì‹œìŠ¤í…œ ì„±ëŠ¥ ë° í’ˆì§ˆ ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ì²´ê³„"
        ]
        
        return '\n'.join(considerations)
    
    def _generate_short_term_improvements(self) -> str:
        """ë‹¨ê¸° ê°œì„ ë°©ì•ˆ ì„¹ì…˜ ìƒì„±"""
        improvements = [
            "ğŸ¯ **ì‘ë‹µ í’ˆì§ˆ ê°œì„ **: í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë° ì»¨í…ìŠ¤íŠ¸ ìµœì í™”",
            "âš¡ **ì†ë„ ìµœì í™”**: ìºì‹± ë° ë°°ì¹˜ ì²˜ë¦¬ ê°œì„ ",
            "ğŸ” **ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ**: ë” ì •êµí•œ ì²­í‚¹ ë° ë©”íƒ€ë°ì´í„° í™œìš©",
            "ğŸ› **ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”**: ì˜ˆì™¸ ìƒí™© ëŒ€ì‘ ë° fallback ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„"
        ]
        
        return '\n'.join(improvements)
    
    def _generate_long_term_roadmap(self) -> str:
        """ì¤‘ì¥ê¸° ë¡œë“œë§µ ì„¹ì…˜ ìƒì„±"""
        roadmap = [
            "ğŸ“š **ë°ì´í„° í™•ì¥**: íŒë¡€, ë²•ë ¹, í•´ì„ë¡€ ë“± ë‹¤ì–‘í•œ ë²•ë¥  ë°ì´í„° ì¶”ê°€",
            "ğŸ¤– **ëª¨ë¸ ê°œì„ **: ë²•ë¥  ë„ë©”ì¸ íŠ¹í™” íŒŒì¸íŠœë‹ ëª¨ë¸ ê°œë°œ",
            "ğŸ”— **ì‹œìŠ¤í…œ í†µí•©**: ê¸°ì¡´ ë²•ë¥  ì •ë³´ ì‹œìŠ¤í…œê³¼ì˜ ì—°ë™",
            "ğŸ‘¥ **ì‚¬ìš©ì ê²½í—˜**: ê°œì¸í™”ëœ ë²•ë¥  ìƒë‹´ ê¸°ëŠ¥ ì¶”ê°€",
            "ğŸ“± **ë©€í‹°í”Œë«í¼**: ëª¨ë°”ì¼ ì•± ë° API ì„œë¹„ìŠ¤ ì œê³µ"
        ]
        
        return '\n'.join(roadmap)
    
    def _generate_test_case_analysis(self) -> str:
        """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¶„ì„ ì„¹ì…˜ ìƒì„±"""
        if not self.quality_results or 'test_results' not in self.quality_results:
            return "âš ï¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¶„ì„ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        test_results = self.quality_results['test_results']
        content = []
        
        if 'individual_results' in test_results:
            content.append(f"**ì´ {len(test_results['individual_results'])}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¶„ì„**")
            content.append("")
            
            # ìƒìœ„ ë° í•˜ìœ„ ì„±ê³¼ ì¼€ì´ìŠ¤ ë¶„ì„
            results = test_results['individual_results']
            if results:
                # ê´€ë ¨ì„± ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
                sorted_results = sorted(results, 
                                      key=lambda x: x['quality_evaluation']['relevance_score'], 
                                      reverse=True)
                
                if len(sorted_results) > 0:
                    best = sorted_results[0]
                    content.append("**ìµœê³  ì„±ê³¼ ì¼€ì´ìŠ¤**:")
                    content.append(f"- ì§ˆë¬¸: {best['question'][:100]}...")
                    content.append(f"- ì¹´í…Œê³ ë¦¬: {best['category']}")
                    content.append(f"- ê´€ë ¨ì„± ì ìˆ˜: {best['quality_evaluation']['relevance_score']}/10")
                    content.append("")
                
                if len(sorted_results) > 1:
                    worst = sorted_results[-1]
                    content.append("**ê°œì„  í•„ìš” ì¼€ì´ìŠ¤**:")
                    content.append(f"- ì§ˆë¬¸: {worst['question'][:100]}...")
                    content.append(f"- ì¹´í…Œê³ ë¦¬: {worst['category']}")
                    content.append(f"- ê´€ë ¨ì„± ì ìˆ˜: {worst['quality_evaluation']['relevance_score']}/10")
                    content.append(f"- ì£¼ìš” ì´ìŠˆ: {', '.join(worst['quality_evaluation']['issues'])}")
        
        return '\n'.join(content)
    
    def _generate_error_analysis(self) -> str:
        """ì˜¤ë¥˜ ë¶„ì„ ì„¹ì…˜ ìƒì„±"""
        total_errors = 0
        error_details = []
        
        # ì„±ëŠ¥ ë¶„ì„ ì˜¤ë¥˜
        if self.performance_results:
            perf_errors = self.performance_results.get('total_errors', 0)
            total_errors += perf_errors
            
            if perf_errors > 0:
                error_details.append(f"- ì„±ëŠ¥ ë¶„ì„ ì˜¤ë¥˜: {perf_errors}ê±´")
        
        # í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜
        if self.quality_results and 'test_results' in self.quality_results:
            quality_errors = self.quality_results['test_results'].get('failed_responses', 0)
            total_errors += quality_errors
            
            if quality_errors > 0:
                error_details.append(f"- ì±—ë´‡ ì‘ë‹µ ì‹¤íŒ¨: {quality_errors}ê±´")
        
        if total_errors == 0:
            return "âœ… ë¶„ì„ ê³¼ì •ì—ì„œ ë°œìƒí•œ ì˜¤ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤."
        else:
            content = [f"âš ï¸ **ì´ {total_errors}ê°œ ì˜¤ë¥˜ ë°œìƒ**", ""]
            content.extend(error_details)
            content.extend([
                "",
                "**ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­**:",
                "- ì‹œìŠ¤í…œ í™˜ê²½ ì„¤ì • ì ê²€",
                "- API í‚¤ ë° ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸",
                "- ì˜¤ë¥˜ ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ê°•í™”"
            ])
            return '\n'.join(content)
    
    def _generate_conclusion(self) -> str:
        """ê²°ë¡  ì„¹ì…˜ ìƒì„±"""
        conclusion = []
        
        # ì „ì²´ í‰ê°€ ë“±ê¸‰
        if (self.quality_results and 'overall_assessment' in self.quality_results and 
            'overall_grade' in self.quality_results['overall_assessment']):
            grade = self.quality_results['overall_assessment']['overall_grade']
            conclusion.append(f"**ì „ì²´ ì‹œìŠ¤í…œ í‰ê°€: {grade}**")
            conclusion.append("")
        
        conclusion.extend([
            "ë³¸ ë¶„ì„ì„ í†µí•´ êµ¬ì¶•ëœ ë²•ë¥  AI ì‹œìŠ¤í…œì€ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ë³´ì…ë‹ˆë‹¤:",
            "",
            "1. **ê¸°ìˆ ì  ì™„ì„±ë„**: ìµœì‹  AI ê¸°ìˆ ì„ í™œìš©í•œ ê²¬ê³ í•œ RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•",
            "2. **ë„ë©”ì¸ íŠ¹í™”**: í•œêµ­ ë²•ë¥  ë°ì´í„°ì— íŠ¹í™”ëœ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±",
            "3. **ì‚¬ìš©ì ì ‘ê·¼ì„±**: ì§ê´€ì ì¸ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•œ ì‰¬ìš´ ì ‘ê·¼",
            "",
            "í–¥í›„ ì§€ì†ì ì¸ ê°œì„ ê³¼ í™•ì¥ì„ í†µí•´ ë”ìš± ì •í™•í•˜ê³  ìœ ìš©í•œ ë²•ë¥  AI ì„œë¹„ìŠ¤ë¡œ ë°œì „í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.",
            "",
            "**ì£¼ì˜ì‚¬í•­**: ë³¸ ì‹œìŠ¤í…œì˜ ë‹µë³€ì€ ì°¸ê³ ìš©ì´ë©°, ì‹¤ì œ ë²•ë¥  ë¬¸ì œì— ëŒ€í•´ì„œëŠ” ë°˜ë“œì‹œ ì „ë¬¸ ë³€í˜¸ì‚¬ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        ])
        
        return '\n'.join(conclusion)
    
    def generate_json_summary(self) -> str:
        """JSON í˜•ì‹ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        summary = {
            "report_metadata": {
                "generated_at": datetime.now().isoformat(),
                "analysis_duration": (datetime.now() - self.analysis_start_time).total_seconds(),
                "report_version": "1.0"
            },
            "performance_summary": self.performance_results,
            "quality_summary": self.quality_results,
            "key_findings": {
                "overall_status": "completed",
                "critical_issues": [],
                "recommendations": []
            }
        }
        
        # ì¤‘ìš” ë°œê²¬ì‚¬í•­ ì¶”ê°€
        if self.performance_results and self.performance_results.get('total_errors', 0) > 0:
            summary["key_findings"]["critical_issues"].append("ì‹œìŠ¤í…œ ì•ˆì •ì„± ê°œì„  í•„ìš”")
        
        if (self.quality_results and 'overall_assessment' in self.quality_results and 
            'recommendations' in self.quality_results['overall_assessment']):
            summary["key_findings"]["recommendations"] = self.quality_results['overall_assessment']['recommendations']
        
        return json.dumps(summary, indent=2, ensure_ascii=False)
    
    def generate_all_reports(self) -> List[str]:
        """ëª¨ë“  ë³´ê³ ì„œ ìƒì„± ë° ì €ì¥"""
        generated_files = []
        
        try:
            # Markdown ë³´ê³ ì„œ ìƒì„±
            markdown_content = self.generate_markdown_report()
            markdown_path = self.report_dir / "system_analysis_report.md"
            with open(markdown_path, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            generated_files.append(str(markdown_path))
            print(f"   âœ… Markdown ë³´ê³ ì„œ ìƒì„±: {markdown_path}")
            
            # JSON ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
            json_content = self.generate_json_summary()
            json_path = self.report_dir / "analysis_summary.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                f.write(json_content)
            generated_files.append(str(json_path))
            print(f"   âœ… JSON ìš”ì•½ ìƒì„±: {json_path}")
            
            # ì„±ëŠ¥ ë°ì´í„° ìƒì„¸ ì €ì¥
            if self.performance_results:
                perf_path = self.report_dir / "performance_metrics.json"
                with open(perf_path, 'w', encoding='utf-8') as f:
                    json.dump(self.performance_results, f, indent=2, ensure_ascii=False)
                generated_files.append(str(perf_path))
                print(f"   âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥: {perf_path}")
            
            # í’ˆì§ˆ ë°ì´í„° ìƒì„¸ ì €ì¥
            if self.quality_results:
                quality_path = self.report_dir / "quality_assessment.json"
                with open(quality_path, 'w', encoding='utf-8') as f:
                    json.dump(self.quality_results, f, indent=2, ensure_ascii=False)
                generated_files.append(str(quality_path))
                print(f"   âœ… í’ˆì§ˆ í‰ê°€ ì €ì¥: {quality_path}")
            
        except Exception as e:
            print(f"   âŒ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise
        
        return generated_files


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°
    dummy_performance = {"test": "performance_data"}
    dummy_quality = {"test": "quality_data"}
    
    generator = ReportGenerator(dummy_performance, dummy_quality, datetime.now())
    files = generator.generate_all_reports()
    
    print("ìƒì„±ëœ ë³´ê³ ì„œ:")
    for file_path in files:
        print(f"  - {file_path}")
#!/usr/bin/env python3
"""
ì†ë„ ìµœì í™” êµ¬í˜„

Task 1ê³¼ Task 2ì˜ í•µì‹¬ ì„±ëŠ¥ ë³‘ëª©ì§€ì ì„ í•´ê²°í•˜ëŠ” ìµœì í™” ê¸°ë²•ë“¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
import asyncio
import json
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from datetime import datetime, timedelta
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import cachetools
import pickle
import hashlib
import numpy as np

# Add parent directories to path
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
sys.path.insert(0, str(parent_dir))

try:
    from task1.app.embedding_client import EmbeddingClient
    from task1.app.db_connection import VectorDB
    from task1.app.config import INDEX_NAME, NAMESPACE
    from task2.app import LawChatbot
except ImportError as e:
    print(f"ëª¨ë“ˆ import ì˜¤ë¥˜: {e}")


class OptimizedEmbeddingClient:
    """ìµœì í™”ëœ ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸
    
    ì£¼ìš” ìµœì í™”:
    1. ì„ë² ë”© ê²°ê³¼ ìºì‹±
    2. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
    3. ë¹„ë™ê¸° ì²˜ë¦¬ ì§€ì›
    4. ì—°ê²° í’€ë§
    """
    
    def __init__(self, original_client: EmbeddingClient, cache_size: int = 1000):
        """
        Args:
            original_client: ì›ë³¸ ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸
            cache_size: ìºì‹œ í¬ê¸° (ì„ë² ë”© ê°œìˆ˜)
        """
        self.original_client = original_client
        
        # LRU ìºì‹œ ì„¤ì •
        self.cache = cachetools.LRUCache(maxsize=cache_size)
        self.cache_hits = 0
        self.cache_misses = 0
        
        # ë°°ì¹˜ ìµœì í™” ì„¤ì •
        self.optimal_batch_size = 8  # API ì œí•œ ê³ ë ¤
        self.max_concurrent_batches = 3
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'total_time': 0,
            'avg_batch_time': 0
        }
    
    def _get_cache_key(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì— ëŒ€í•œ ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def embed_with_cache(self, texts: List[str]) -> Dict[str, Any]:
        """ìºì‹œë¥¼ í™œìš©í•œ ì„ë² ë”© ìƒì„±
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì„ë² ë”© ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        # ìºì‹œì—ì„œ ê¸°ì¡´ ì„ë² ë”© í™•ì¸
        cached_embeddings = {}
        uncached_texts = []
        uncached_indices = []
        
        for i, text in enumerate(texts):
            cache_key = self._get_cache_key(text)
            if cache_key in self.cache:
                cached_embeddings[i] = self.cache[cache_key]
                self.cache_hits += 1
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
                self.cache_misses += 1
        
        # ìºì‹œë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”© ìƒì„±
        new_embeddings = {}
        if uncached_texts:
            print(f"ìºì‹œ ë¯¸ìŠ¤: {len(uncached_texts)}ê°œ, ìºì‹œ íˆíŠ¸: {len(cached_embeddings)}ê°œ")
            result = self.original_client.embed(uncached_texts, batch_size=self.optimal_batch_size)
            
            # ìƒˆë¡œìš´ ì„ë² ë”©ì„ ìºì‹œì— ì €ì¥
            for i, (text, embedding) in enumerate(zip(uncached_texts, result['embeddings'])):
                cache_key = self._get_cache_key(text)
                self.cache[cache_key] = embedding.tolist()
                new_embeddings[uncached_indices[i]] = embedding.tolist()
        
        # ê²°ê³¼ ì¡°í•©
        final_embeddings = []
        for i in range(len(texts)):
            if i in cached_embeddings:
                final_embeddings.append(cached_embeddings[i])
            else:
                final_embeddings.append(new_embeddings[i])
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        end_time = time.time()
        self.stats['total_requests'] += len(texts)
        self.stats['cache_hits'] = self.cache_hits
        self.stats['cache_misses'] = self.cache_misses
        self.stats['total_time'] += (end_time - start_time)
        
        return {
            'embeddings': final_embeddings,
            'dim': len(final_embeddings[0]) if final_embeddings else 0,
            'cache_hit_rate': self.cache_hits / (self.cache_hits + self.cache_misses)
        }
    
    def embed_query_with_cache(self, text: str):
        """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”© (ìºì‹œ ì ìš©)"""
        result = self.embed_with_cache([text])
        return result['embeddings'][0]
    
    async def embed_async(self, texts: List[str]) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì„ë² ë”© ìƒì„±"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.embed_with_cache, texts)
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = self.cache_hits + self.cache_misses
        if total_requests > 0:
            hit_rate = self.cache_hits / total_requests
        else:
            hit_rate = 0
        
        return {
            'cache_size': len(self.cache),
            'max_cache_size': self.cache.maxsize,
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': hit_rate,
            'total_requests': self.stats['total_requests']
        }


class OptimizedVectorDB:
    """ìµœì í™”ëœ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í´ë¼ì´ì–¸íŠ¸
    
    ì£¼ìš” ìµœì í™”:
    1. ê²€ìƒ‰ ê²°ê³¼ ìºì‹±
    2. ë°°ì¹˜ ê²€ìƒ‰ ì§€ì›
    3. ì—°ê²° í’€ë§
    4. ì§€ëŠ¥í˜• ì¿¼ë¦¬ ìµœì í™”
    """
    
    def __init__(self, original_db: VectorDB, cache_ttl: int = 300):
        """
        Args:
            original_db: ì›ë³¸ ë²¡í„° DB í´ë¼ì´ì–¸íŠ¸
            cache_ttl: ìºì‹œ TTL (ì´ˆ)
        """
        self.original_db = original_db
        
        # TTL ìºì‹œ ì„¤ì • (5ë¶„)
        self.search_cache = cachetools.TTLCache(maxsize=500, ttl=cache_ttl)
        self.cache_hits = 0
        self.cache_misses = 0
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            'total_searches': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'avg_search_time': 0
        }
    
    def _get_search_cache_key(self, query_vector: List[float], top_k: int, namespace: str = None) -> str:
        """ê²€ìƒ‰ ì¿¼ë¦¬ì— ëŒ€í•œ ìºì‹œ í‚¤ ìƒì„±"""
        # ë²¡í„°ë¥¼ ê°„ë‹¨í•œ í•´ì‹œë¡œ ë³€í™˜ (ì •í™•ë„ vs ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„)
        vector_hash = hashlib.md5(str(query_vector[:10]).encode()).hexdigest()
        return f"{vector_hash}_{top_k}_{namespace or 'default'}"
    
    def search_with_cache(self, query_vector: Union[List[float], np.ndarray], 
                         top_k: int = 5, namespace: str = None) -> Any:
        """ìºì‹œë¥¼ í™œìš©í•œ ë²¡í„° ê²€ìƒ‰
        
        Args:
            query_vector: ì¿¼ë¦¬ ë²¡í„°
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            namespace: ê²€ìƒ‰í•  ë„¤ì„ìŠ¤í˜ì´ìŠ¤
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        start_time = time.time()
        
        # ë²¡í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if hasattr(query_vector, 'tolist'):
            query_vector = query_vector.tolist()
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._get_search_cache_key(query_vector, top_k, namespace)
        
        # ìºì‹œ í™•ì¸
        if cache_key in self.search_cache:
            self.cache_hits += 1
            result = self.search_cache[cache_key]
            print(f"ğŸ¯ ë²¡í„° ê²€ìƒ‰ ìºì‹œ íˆíŠ¸")
        else:
            self.cache_misses += 1
            result = self.original_db.search(query_vector, top_k, namespace)
            self.search_cache[cache_key] = result
            print(f"ğŸ” ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰ (ìºì‹œ ë¯¸ìŠ¤)")
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        end_time = time.time()
        search_time = end_time - start_time
        self.stats['total_searches'] += 1
        self.stats['cache_hits'] = self.cache_hits
        self.stats['cache_misses'] = self.cache_misses
        
        if self.stats['avg_search_time'] == 0:
            self.stats['avg_search_time'] = search_time
        else:
            self.stats['avg_search_time'] = (self.stats['avg_search_time'] + search_time) / 2
        
        return result
    
    async def search_async(self, query_vector: Union[List[float], np.ndarray], 
                          top_k: int = 5, namespace: str = None) -> Any:
        """ë¹„ë™ê¸° ë²¡í„° ê²€ìƒ‰"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.search_with_cache, query_vector, top_k, namespace)
    
    def batch_search(self, query_vectors: List[List[float]], top_k: int = 5, 
                    namespace: str = None) -> List[Any]:
        """ë°°ì¹˜ ë²¡í„° ê²€ìƒ‰"""
        results = []
        with ThreadPoolExecutor(max_workers=3) as executor:
            future_to_vector = {
                executor.submit(self.search_with_cache, vector, top_k, namespace): vector 
                for vector in query_vectors
            }
            
            for future in as_completed(future_to_vector):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    print(f"ë°°ì¹˜ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                    results.append(None)
        
        return results
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = self.cache_hits + self.cache_misses
        if total_requests > 0:
            hit_rate = self.cache_hits / total_requests
        else:
            hit_rate = 0
        
        return {
            'cache_size': len(self.search_cache),
            'max_cache_size': self.search_cache.maxsize,
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': hit_rate,
            'avg_search_time': self.stats['avg_search_time']
        }


class OptimizedLawChatbot:
    """ìµœì í™”ëœ ë²•ë¥  ì±—ë´‡
    
    ì£¼ìš” ìµœì í™”:
    1. ì „ì²´ íŒŒì´í”„ë¼ì¸ ìºì‹±
    2. ë³‘ë ¬ ì²˜ë¦¬
    3. ì§€ëŠ¥í˜• ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬
    4. ì‘ë‹µ í’ˆì§ˆ ê°œì„ 
    """
    
    def __init__(self, original_chatbot: LawChatbot):
        """
        Args:
            original_chatbot: ì›ë³¸ ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤
        """
        self.original_chatbot = original_chatbot
        
        # ìµœì í™”ëœ ì»´í¬ë„ŒíŠ¸ë¡œ êµì²´
        self.optimized_embedder = OptimizedEmbeddingClient(original_chatbot.embedder)
        self.optimized_vector_db = OptimizedVectorDB(original_chatbot.vector_db)
        
        # ì‘ë‹µ ìºì‹± (ì§ˆë¬¸-ì‘ë‹µ ìŒ)
        self.response_cache = cachetools.TTLCache(maxsize=200, ttl=1800)  # 30ë¶„
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            'total_queries': 0,
            'avg_response_time': 0,
            'cache_hit_rate': 0
        }
    
    def _get_response_cache_key(self, query: str) -> str:
        """ì‘ë‹µ ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(query.strip().lower().encode('utf-8')).hexdigest()
    
    async def retrieve_relevant_docs_async(self, query: str, top_k: int = 3) -> List[str]:
        """ë¹„ë™ê¸° ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            # ì„ë² ë”© ìƒì„±
            query_embedding = await self.optimized_embedder.embed_async([query])
            query_vector = query_embedding['embeddings'][0]
            
            # ë²¡í„° ê²€ìƒ‰
            results = await self.optimized_vector_db.search_async(
                query_vector=query_vector,
                top_k=top_k
            )
            
            # ë¬¸ì„œ ì¶”ì¶œ
            docs = []
            for match in results.matches:
                metadata = match.metadata or {}
                content = metadata.get('text', '') or metadata.get('content', '') or metadata.get('íŒê²°ìš”ì§€', '')
                case_name = metadata.get('ì‚¬ê±´ëª…', '')
                case_number = metadata.get('ì‚¬ê±´ë²ˆí˜¸', '')
                
                if content:
                    doc_text = f"[ì‚¬ê±´: {case_name} ({case_number})]\n{content}"
                    docs.append(doc_text)
            
            return docs
            
        except Exception as e:
            print(f"ë¹„ë™ê¸° ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def generate_response_optimized(self, query: str, context_docs: List[str]) -> str:
        """ìµœì í™”ëœ ì‘ë‹µ ìƒì„±"""
        try:
            # ì»¨í…ìŠ¤íŠ¸ ìµœì í™” (ì¤‘ë³µ ì œê±°, ê¸¸ì´ ì œí•œ)
            optimized_context = self._optimize_context(context_docs)
            
            # ê¸°ì¡´ ì‘ë‹µ ìƒì„± ë¡œì§ ì‚¬ìš©
            return self.original_chatbot.generate_response(query, optimized_context)
            
        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def _optimize_context(self, docs: List[str], max_length: int = 3000) -> List[str]:
        """ì»¨í…ìŠ¤íŠ¸ ìµœì í™”"""
        # ì¤‘ë³µ ì œê±°
        unique_docs = list(dict.fromkeys(docs))
        
        # ê¸¸ì´ ì œí•œ
        optimized_docs = []
        total_length = 0
        
        for doc in unique_docs:
            if total_length + len(doc) <= max_length:
                optimized_docs.append(doc)
                total_length += len(doc)
            else:
                # ë‚¨ì€ ê³µê°„ì— ë§ê²Œ ìë¥´ê¸°
                remaining = max_length - total_length
                if remaining > 100:  # ìµœì†Œ 100ìëŠ” í¬í•¨
                    optimized_docs.append(doc[:remaining] + "...")
                break
        
        return optimized_docs
    
    async def chat_optimized(self, message: str, history: List[dict]) -> tuple[str, List[dict]]:
        """ìµœì í™”ëœ ì±—ë´‡ ëŒ€í™”"""
        if not message.strip():
            return "", history
        
        start_time = time.time()
        
        try:
            # ì‘ë‹µ ìºì‹œ í™•ì¸
            cache_key = self._get_response_cache_key(message)
            
            if cache_key in self.response_cache:
                print("ğŸ’° ì‘ë‹µ ìºì‹œ íˆíŠ¸")
                response = self.response_cache[cache_key]
            else:
                print("ğŸ”„ ìƒˆë¡œìš´ ì‘ë‹µ ìƒì„±")
                # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¬¸ì„œ ê²€ìƒ‰ê³¼ ê¸°íƒ€ ì‘ì—… ìˆ˜í–‰
                relevant_docs = await self.retrieve_relevant_docs_async(message, top_k=3)
                response = self.generate_response_optimized(message, relevant_docs)
                
                # ì‘ë‹µ ìºì‹±
                self.response_cache[cache_key] = response
            
            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": response})
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            end_time = time.time()
            response_time = end_time - start_time
            self.stats['total_queries'] += 1
            
            if self.stats['avg_response_time'] == 0:
                self.stats['avg_response_time'] = response_time
            else:
                self.stats['avg_response_time'] = (self.stats['avg_response_time'] + response_time) / 2
            
            return "", history
            
        except Exception as e:
            error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": error_msg})
            return "", history
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """ìµœì í™” í†µê³„ ë°˜í™˜"""
        return {
            'chatbot': self.stats,
            'embedding': self.optimized_embedder.get_cache_stats(),
            'vector_db': self.optimized_vector_db.get_cache_stats(),
            'response_cache_size': len(self.response_cache)
        }


class SpeedOptimizer:
    """ì†ë„ ìµœì í™” ê´€ë¦¬ì"""
    
    def __init__(self):
        """ìµœì í™” ê´€ë¦¬ì ì´ˆê¸°í™”"""
        self.original_components = {}
        self.optimized_components = {}
        self.benchmark_results = {}
    
    def setup_optimizations(self) -> Dict[str, Any]:
        """ìµœì í™” ì„¤ì • ì ìš©"""
        print("ğŸš€ ì†ë„ ìµœì í™” ì„¤ì • ì¤‘...")
        
        try:
            # 1. ì›ë³¸ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
            print("   - ì›ë³¸ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”...")
            self.original_components['embedder'] = EmbeddingClient()
            self.original_components['vector_db'] = VectorDB(dim=3072)
            self.original_components['chatbot'] = LawChatbot()
            
            # 2. ìµœì í™”ëœ ì»´í¬ë„ŒíŠ¸ ìƒì„±
            print("   - ìµœì í™”ëœ ì»´í¬ë„ŒíŠ¸ ìƒì„±...")
            self.optimized_components['embedder'] = OptimizedEmbeddingClient(
                self.original_components['embedder']
            )
            self.optimized_components['vector_db'] = OptimizedVectorDB(
                self.original_components['vector_db']
            )
            self.optimized_components['chatbot'] = OptimizedLawChatbot(
                self.original_components['chatbot']
            )
            
            print("âœ… ìµœì í™” ì„¤ì • ì™„ë£Œ")
            return {'status': 'success', 'message': 'ëª¨ë“  ìµœì í™” ì»´í¬ë„ŒíŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.'}
            
        except Exception as e:
            print(f"âŒ ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {e}")
            return {'status': 'error', 'message': str(e)}
    
    async def run_performance_comparison(self, test_data: Dict) -> Dict[str, Any]:
        """ì›ë³¸ vs ìµœì í™” ì„±ëŠ¥ ë¹„êµ"""
        print("\nğŸ“Š ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'original': {},
            'optimized': {},
            'improvement': {}
        }
        
        # 1. ì„ë² ë”© ì„±ëŠ¥ ë¹„êµ
        print("\nğŸ¤– ì„ë² ë”© ì„±ëŠ¥ ë¹„êµ...")
        results['original']['embedding'] = await self._benchmark_embedding_original(test_data['test_texts'])
        results['optimized']['embedding'] = await self._benchmark_embedding_optimized(test_data['test_texts'])
        
        # 2. ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ
        print("\nğŸ” ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ...")
        results['original']['search'] = await self._benchmark_search_original(test_data['test_queries'])
        results['optimized']['search'] = await self._benchmark_search_optimized(test_data['test_queries'])
        
        # 3. ì—”ë“œíˆ¬ì—”ë“œ ì„±ëŠ¥ ë¹„êµ
        print("\nğŸ”„ ì±—ë´‡ ì„±ëŠ¥ ë¹„êµ...")
        results['original']['chatbot'] = await self._benchmark_chatbot_original(test_data['test_texts'][:3])
        results['optimized']['chatbot'] = await self._benchmark_chatbot_optimized(test_data['test_texts'][:3])
        
        # 4. ê°œì„ ë„ ê³„ì‚°
        results['improvement'] = self._calculate_improvements(results['original'], results['optimized'])
        
        return results
    
    async def _benchmark_embedding_original(self, test_texts: List[str]) -> Dict[str, float]:
        """ì›ë³¸ ì„ë² ë”© ì„±ëŠ¥ ì¸¡ì •"""
        start_time = time.time()
        
        for text in test_texts[:3]:  # ìƒ˜í”Œë§Œ í…ŒìŠ¤íŠ¸
            self.original_components['embedder'].embed([text])
        
        end_time = time.time()
        total_time = end_time - start_time
        
        return {
            'total_time': total_time,
            'avg_time_per_text': total_time / 3,
            'throughput': 3 / total_time
        }
    
    async def _benchmark_embedding_optimized(self, test_texts: List[str]) -> Dict[str, float]:
        """ìµœì í™” ì„ë² ë”© ì„±ëŠ¥ ì¸¡ì •"""
        start_time = time.time()
        
        # ì²« ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ ë¯¸ìŠ¤)
        await self.optimized_components['embedder'].embed_async(test_texts[:3])
        
        # ë‘ ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ íˆíŠ¸)
        await self.optimized_components['embedder'].embed_async(test_texts[:3])
        
        end_time = time.time()
        total_time = end_time - start_time
        
        cache_stats = self.optimized_components['embedder'].get_cache_stats()
        
        return {
            'total_time': total_time,
            'avg_time_per_text': total_time / 6,  # 6ê°œ ìš”ì²­ (3 x 2)
            'throughput': 6 / total_time,
            'cache_hit_rate': cache_stats['hit_rate']
        }
    
    async def _benchmark_search_original(self, test_queries: List[str]) -> Dict[str, float]:
        """ì›ë³¸ ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •"""
        embedder = self.original_components['embedder']
        vector_db = self.original_components['vector_db']
        
        start_time = time.time()
        
        for query in test_queries[:3]:
            query_embedding = embedder.embed_query(query)
            vector_db.search(query_embedding, top_k=5)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        return {
            'total_time': total_time,
            'avg_time_per_query': total_time / 3,
            'queries_per_second': 3 / total_time
        }
    
    async def _benchmark_search_optimized(self, test_queries: List[str]) -> Dict[str, float]:
        """ìµœì í™” ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •"""
        embedder = self.optimized_components['embedder']
        vector_db = self.optimized_components['vector_db']
        
        start_time = time.time()
        
        # ì²« ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ ë¯¸ìŠ¤)
        for query in test_queries[:3]:
            query_embedding = embedder.embed_query_with_cache(query)
            await vector_db.search_async(query_embedding, top_k=5)
        
        # ë‘ ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ íˆíŠ¸)
        for query in test_queries[:3]:
            query_embedding = embedder.embed_query_with_cache(query)
            await vector_db.search_async(query_embedding, top_k=5)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        cache_stats = vector_db.get_cache_stats()
        
        return {
            'total_time': total_time,
            'avg_time_per_query': total_time / 6,  # 6ê°œ ìš”ì²­ (3 x 2)
            'queries_per_second': 6 / total_time,
            'cache_hit_rate': cache_stats['hit_rate']
        }
    
    async def _benchmark_chatbot_original(self, test_questions: List[str]) -> Dict[str, float]:
        """ì›ë³¸ ì±—ë´‡ ì„±ëŠ¥ ì¸¡ì •"""
        chatbot = self.original_components['chatbot']
        
        start_time = time.time()
        
        for question in test_questions:
            history = []
            chatbot.chat(question, history)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        return {
            'total_time': total_time,
            'avg_response_time': total_time / len(test_questions),
            'questions_per_minute': len(test_questions) / (total_time / 60)
        }
    
    async def _benchmark_chatbot_optimized(self, test_questions: List[str]) -> Dict[str, float]:
        """ìµœì í™” ì±—ë´‡ ì„±ëŠ¥ ì¸¡ì •"""
        chatbot = self.optimized_components['chatbot']
        
        start_time = time.time()
        
        for question in test_questions:
            history = []
            await chatbot.chat_optimized(question, history)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        stats = chatbot.get_optimization_stats()
        
        return {
            'total_time': total_time,
            'avg_response_time': total_time / len(test_questions),
            'questions_per_minute': len(test_questions) / (total_time / 60),
            'optimization_stats': stats
        }
    
    def _calculate_improvements(self, original: Dict, optimized: Dict) -> Dict[str, Dict]:
        """ê°œì„ ë„ ê³„ì‚°"""
        improvements = {}
        
        for category in original:
            if category in optimized:
                improvements[category] = {}
                
                for metric in original[category]:
                    if isinstance(original[category][metric], (int, float)) and \
                       metric in optimized[category] and \
                       isinstance(optimized[category][metric], (int, float)):
                        
                        orig_val = original[category][metric]
                        opt_val = optimized[category][metric]
                        
                        if orig_val > 0:
                            # ì‹œê°„ ì§€í‘œëŠ” ê°ì†Œê°€ ê°œì„ , ì²˜ë¦¬ëŸ‰ ì§€í‘œëŠ” ì¦ê°€ê°€ ê°œì„ 
                            if 'time' in metric.lower():
                                improvement = ((orig_val - opt_val) / orig_val) * 100
                            else:
                                improvement = ((opt_val - orig_val) / orig_val) * 100
                            
                            improvements[category][metric] = {
                                'original': orig_val,
                                'optimized': opt_val,
                                'improvement_percent': improvement
                            }
        
        return improvements
    
    def save_benchmark_results(self, results: Dict, filepath: Optional[str] = None):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥"""
        if not filepath:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = current_dir / "reports" / f"optimization_benchmark_{timestamp}.json"
        
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥: {filepath}")
        return filepath


def create_test_data():
    """í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±"""
    return {
        'test_texts': [
            "ê·¼ë¡œê³„ì•½ í•´ì§€ì— ê´€í•œ ë²•ì  ì ˆì°¨ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "ë¶€ë™ì‚° ì†Œìœ ê¶Œ ì´ì „ ë“±ê¸° ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            "êµí†µì‚¬ê³  ë°œìƒ ì‹œ ì†í•´ë°°ìƒ ì±…ì„ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.",
            "ì„ê¸ˆì²´ë¶ˆ ì‹œ ê·¼ë¡œìê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ë²•ì  ì¡°ì¹˜ëŠ”?",
            "ê³„ì•½ì„œ ì‘ì„± ì‹œ í•„ìˆ˜ì ìœ¼ë¡œ í¬í•¨í•´ì•¼ í•  ë‚´ìš©ì€?"
        ],
        'test_queries': [
            "ê·¼ë¡œê³„ì•½ í•´ì§€",
            "ë¶€ë™ì‚° ì†Œìœ ê¶Œ",
            "êµí†µì‚¬ê³  ì†í•´ë°°ìƒ",
            "ì„ê¸ˆì²´ë¶ˆ",
            "ê³„ì•½ì„œ ì‘ì„±"
        ]
    }


# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì†ë„ ìµœì í™” ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 50)
    
    # ìµœì í™” ê´€ë¦¬ì ì´ˆê¸°í™”
    optimizer = SpeedOptimizer()
    
    # ìµœì í™” ì„¤ì •
    setup_result = optimizer.setup_optimizations()
    if setup_result['status'] != 'success':
        print(f"âŒ ìµœì í™” ì„¤ì • ì‹¤íŒ¨: {setup_result['message']}")
        return
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_data = {
        'test_texts': [
            "ê·¼ë¡œê³„ì•½ í•´ì§€ì— ê´€í•œ ë²•ì  ì ˆì°¨ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "ë¶€ë™ì‚° ì†Œìœ ê¶Œ ì´ì „ ë“±ê¸° ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            "êµí†µì‚¬ê³  ë°œìƒ ì‹œ ì†í•´ë°°ìƒ ì±…ì„ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”."
        ],
        'test_queries': [
            "ê·¼ë¡œê³„ì•½ í•´ì§€",
            "ë¶€ë™ì‚° ì†Œìœ ê¶Œ",
            "êµí†µì‚¬ê³  ì†í•´ë°°ìƒ"
        ]
    }
    
    # ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰
    results = await optimizer.run_performance_comparison(test_data)
    
    # ê²°ê³¼ ì €ì¥
    optimizer.save_benchmark_results(results)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š ìµœì í™” ê²°ê³¼ ìš”ì•½:")
    print("=" * 50)
    
    for category, improvements in results['improvement'].items():
        print(f"\nğŸ“ˆ {category.upper()} ê°œì„ ë„:")
        for metric, data in improvements.items():
            improvement = data['improvement_percent']
            status = "ğŸŸ¢" if improvement > 0 else "ğŸ”´"
            print(f"   {status} {metric}: {improvement:+.1f}%")


if __name__ == "__main__":
    asyncio.run(main())
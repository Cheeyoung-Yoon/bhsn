#!/usr/bin/env python3
"""
ì†ë„ ìµœì í™” ì§€í‘œ ì •ì˜ ë° ì¸¡ì • ì‹œìŠ¤í…œ

Task 1ê³¼ Task 2ì˜ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ í•µì‹¬ ì§€í‘œë“¤ì„ ì •ì˜í•˜ê³  ì¸¡ì •í•©ë‹ˆë‹¤.
"""

import time
import asyncio
from typing import Dict, List, Optional, Callable, Any
from dataclasses import dataclass
from datetime import datetime
import statistics
import json


@dataclass
class PerformanceMetric:
    """ì„±ëŠ¥ ì¸¡ì • ì§€í‘œ í´ë˜ìŠ¤"""
    name: str
    value: float
    unit: str
    timestamp: datetime
    category: str
    description: str


class SpeedOptimizationMetrics:
    """ì†ë„ ìµœì í™” ì§€í‘œ ì¸¡ì • ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.metrics = []
        self.baselines = {}
        
    # =================
    # 1. ì„ë² ë”© ì„±ëŠ¥ ì§€í‘œ
    # =================
    
    def measure_embedding_throughput(self, embedder, test_texts: List[str]) -> Dict[str, float]:
        """ì„ë² ë”© ì²˜ë¦¬ëŸ‰ ì¸¡ì •
        
        ì§€í‘œ:
        - texts_per_second: ì´ˆë‹¹ ì²˜ë¦¬ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ìˆ˜
        - avg_time_per_text: í…ìŠ¤íŠ¸ ë‹¹ í‰ê·  ì²˜ë¦¬ ì‹œê°„
        - batch_efficiency: ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ì„±
        """
        metrics = {}
        
        # 1. ê°œë³„ ì²˜ë¦¬ ì„±ëŠ¥
        start_time = time.time()
        for text in test_texts[:3]:  # ìƒ˜í”Œ 3ê°œ
            embedder.embed([text])
        end_time = time.time()
        
        total_time = end_time - start_time
        metrics['individual_avg_time'] = total_time / 3
        metrics['individual_throughput'] = 3 / total_time
        
        # 2. ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥
        batch_sizes = [1, 3, 5]
        batch_metrics = {}
        
        for batch_size in batch_sizes:
            if len(test_texts) >= batch_size:
                batch_texts = test_texts[:batch_size]
                start_time = time.time()
                embedder.embed(batch_texts)
                end_time = time.time()
                
                batch_time = end_time - start_time
                batch_metrics[batch_size] = {
                    'total_time': batch_time,
                    'time_per_text': batch_time / batch_size,
                    'throughput': batch_size / batch_time
                }
        
        metrics['batch_performance'] = batch_metrics
        
        # ë°°ì¹˜ íš¨ìœ¨ì„± ê³„ì‚° (ë°°ì¹˜ ì²˜ë¦¬ê°€ ê°œë³„ ì²˜ë¦¬ë³´ë‹¤ ì–¼ë§ˆë‚˜ íš¨ìœ¨ì ì¸ê°€)
        if 5 in batch_metrics and metrics['individual_avg_time'] > 0:
            batch_5_time_per_text = batch_metrics[5]['time_per_text']
            metrics['batch_efficiency'] = metrics['individual_avg_time'] / batch_5_time_per_text
        
        return metrics
    
    def measure_embedding_latency(self, embedder, test_text: str, iterations: int = 5) -> Dict[str, float]:
        """ì„ë² ë”© ì§€ì—°ì‹œê°„ ì¸¡ì •
        
        ì§€í‘œ:
        - min_latency: ìµœì†Œ ì§€ì—°ì‹œê°„
        - max_latency: ìµœëŒ€ ì§€ì—°ì‹œê°„  
        - avg_latency: í‰ê·  ì§€ì—°ì‹œê°„
        - p95_latency: 95 í¼ì„¼íƒ€ì¼ ì§€ì—°ì‹œê°„
        """
        latencies = []
        
        for _ in range(iterations):
            start_time = time.time()
            embedder.embed([test_text])
            end_time = time.time()
            latencies.append(end_time - start_time)
        
        return {
            'min_latency': min(latencies),
            'max_latency': max(latencies),
            'avg_latency': statistics.mean(latencies),
            'p95_latency': sorted(latencies)[int(0.95 * len(latencies))],
            'latency_std': statistics.stdev(latencies) if len(latencies) > 1 else 0
        }
    
    # =================
    # 2. ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥ ì§€í‘œ
    # =================
    
    def measure_search_performance(self, vector_db, embedder, test_queries: List[str]) -> Dict[str, float]:
        """ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •
        
        ì§€í‘œ:
        - queries_per_second: ì´ˆë‹¹ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì¿¼ë¦¬ ìˆ˜
        - avg_search_time: í‰ê·  ê²€ìƒ‰ ì‹œê°„
        - search_accuracy: ê²€ìƒ‰ ì •í™•ë„ (ê´€ë ¨ì„±)
        """
        search_times = []
        total_results = 0
        
        for query in test_queries:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = embedder.embed_query(query)
            
            # ê²€ìƒ‰ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            results = vector_db.search(query_embedding, top_k=5)
            end_time = time.time()
            
            search_time = end_time - start_time
            search_times.append(search_time)
            total_results += len(results.matches) if hasattr(results, 'matches') else 0
        
        return {
            'avg_search_time': statistics.mean(search_times),
            'min_search_time': min(search_times),
            'max_search_time': max(search_times),
            'queries_per_second': len(test_queries) / sum(search_times),
            'avg_results_count': total_results / len(test_queries),
            'search_time_std': statistics.stdev(search_times) if len(search_times) > 1 else 0
        }
    
    # =================
    # 3. ì—”ë“œíˆ¬ì—”ë“œ ì„±ëŠ¥ ì§€í‘œ
    # =================
    
    def measure_rag_pipeline_performance(self, chatbot, test_questions: List[str]) -> Dict[str, float]:
        """RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ ì„±ëŠ¥ ì¸¡ì •
        
        ì§€í‘œ:
        - total_response_time: ì „ì²´ ì‘ë‹µ ì‹œê°„
        - retrieval_time: ë¬¸ì„œ ê²€ìƒ‰ ì‹œê°„
        - generation_time: ì‘ë‹µ ìƒì„± ì‹œê°„
        - questions_per_minute: ë¶„ë‹¹ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì§ˆë¬¸ ìˆ˜
        """
        pipeline_metrics = []
        
        for question in test_questions:
            metrics = {}
            
            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œê°„ ì¸¡ì •
            total_start = time.time()
            
            # 1. ë¬¸ì„œ ê²€ìƒ‰ ì‹œê°„
            retrieval_start = time.time()
            relevant_docs = chatbot.retrieve_relevant_docs(question, top_k=3)
            retrieval_end = time.time()
            metrics['retrieval_time'] = retrieval_end - retrieval_start
            
            # 2. ì‘ë‹µ ìƒì„± ì‹œê°„
            generation_start = time.time()
            response = chatbot.generate_response(question, relevant_docs)
            generation_end = time.time()
            metrics['generation_time'] = generation_end - generation_start
            
            total_end = time.time()
            metrics['total_response_time'] = total_end - total_start
            metrics['response_length'] = len(response) if response else 0
            
            pipeline_metrics.append(metrics)
        
        # í†µê³„ ê³„ì‚°
        total_times = [m['total_response_time'] for m in pipeline_metrics]
        retrieval_times = [m['retrieval_time'] for m in pipeline_metrics]
        generation_times = [m['generation_time'] for m in pipeline_metrics]
        
        return {
            'avg_total_response_time': statistics.mean(total_times),
            'avg_retrieval_time': statistics.mean(retrieval_times),
            'avg_generation_time': statistics.mean(generation_times),
            'questions_per_minute': 60 / statistics.mean(total_times),
            'total_time_std': statistics.stdev(total_times) if len(total_times) > 1 else 0,
            'retrieval_time_ratio': statistics.mean(retrieval_times) / statistics.mean(total_times),
            'generation_time_ratio': statistics.mean(generation_times) / statistics.mean(total_times)
        }
    
    # =================
    # 4. ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ì§€í‘œ
    # =================
    
    def measure_memory_usage(self, operation_func: Callable, *args) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
        import tracemalloc
        
        tracemalloc.start()
        
        # ì‘ì—… ì‹¤í–‰
        result = operation_func(*args)
        
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        return {
            'current_memory_mb': current / 1024 / 1024,
            'peak_memory_mb': peak / 1024 / 1024,
            'memory_efficiency': len(str(result)) / (peak / 1024) if peak > 0 else 0  # bytes per KB
        }
    
    # =================
    # 5. ìµœì í™” ëª©í‘œ ì§€í‘œ
    # =================
    
    def set_performance_targets(self) -> Dict[str, Dict[str, float]]:
        """ì„±ëŠ¥ ìµœì í™” ëª©í‘œ ì„¤ì •
        
        Returns:
            ì¹´í…Œê³ ë¦¬ë³„ ëª©í‘œ ì„±ëŠ¥ ì§€í‘œ
        """
        return {
            'embedding': {
                'target_throughput': 10.0,  # texts/second
                'target_latency': 0.5,      # seconds
                'target_batch_efficiency': 3.0  # ë°°ì¹˜ê°€ ê°œë³„ë³´ë‹¤ 3ë°° íš¨ìœ¨ì 
            },
            'search': {
                'target_search_time': 0.1,   # seconds
                'target_qps': 20.0,          # queries/second
                'target_accuracy': 0.8       # precision@5
            },
            'end_to_end': {
                'target_response_time': 3.0,  # seconds
                'target_qpm': 20.0,           # questions/minute
                'target_retrieval_ratio': 0.3 # ê²€ìƒ‰ì´ ì „ì²´ì˜ 30% ì´í•˜
            },
            'resource': {
                'target_memory_mb': 500,      # MB
                'target_memory_efficiency': 1000  # bytes per KB
            }
        }
    
    def calculate_optimization_score(self, current_metrics: Dict, targets: Dict) -> Dict[str, float]:
        """ìµœì í™” ì ìˆ˜ ê³„ì‚°
        
        Args:
            current_metrics: í˜„ì¬ ì¸¡ì •ëœ ì„±ëŠ¥ ì§€í‘œ
            targets: ëª©í‘œ ì„±ëŠ¥ ì§€í‘œ
            
        Returns:
            ì¹´í…Œê³ ë¦¬ë³„ ìµœì í™” ì ìˆ˜ (0-100)
        """
        scores = {}
        
        for category, target_metrics in targets.items():
            if category not in current_metrics:
                scores[category] = 0
                continue
            
            category_scores = []
            current_category = current_metrics[category]
            
            for metric, target_value in target_metrics.items():
                if metric in current_category:
                    current_value = current_category[metric]
                    
                    # ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ì§€í‘œ (throughput, qps, efficiency ë“±)
                    if 'throughput' in metric or 'qps' in metric or 'efficiency' in metric:
                        score = min(100, (current_value / target_value) * 100)
                    # ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ì§€í‘œ (latency, time ë“±)
                    else:
                        score = min(100, (target_value / current_value) * 100)
                    
                    category_scores.append(score)
            
            scores[category] = statistics.mean(category_scores) if category_scores else 0
        
        # ì „ì²´ í‰ê·  ì ìˆ˜
        scores['overall'] = statistics.mean(list(scores.values())) if scores else 0
        
        return scores
    
    # =================
    # 6. ë¹„êµ ë° ë³´ê³ 
    # =================
    
    def save_baseline(self, metrics: Dict[str, Any], name: str = "baseline"):
        """ê¸°ì¤€ì„  ì„±ëŠ¥ ì €ì¥"""
        self.baselines[name] = {
            'metrics': metrics,
            'timestamp': datetime.now().isoformat()
        }
    
    def compare_with_baseline(self, current_metrics: Dict, baseline_name: str = "baseline") -> Dict[str, Dict]:
        """ê¸°ì¤€ì„ ê³¼ í˜„ì¬ ì„±ëŠ¥ ë¹„êµ"""
        if baseline_name not in self.baselines:
            return {"error": f"Baseline '{baseline_name}' not found"}
        
        baseline_metrics = self.baselines[baseline_name]['metrics']
        comparison = {}
        
        def compare_nested(current, baseline, path=""):
            result = {}
            for key, current_value in current.items():
                if isinstance(current_value, dict):
                    if key in baseline and isinstance(baseline[key], dict):
                        result[key] = compare_nested(current_value, baseline[key], f"{path}.{key}")
                elif key in baseline and isinstance(baseline[key], (int, float)):
                    baseline_value = baseline[key]
                    if baseline_value != 0:
                        improvement = ((current_value - baseline_value) / baseline_value) * 100
                        result[key] = {
                            'current': current_value,
                            'baseline': baseline_value,
                            'improvement_percent': improvement,
                            'better': improvement > 0 if 'throughput' in key or 'qps' in key or 'efficiency' in key
                                     else improvement < 0  # ì‹œê°„ ì§€í‘œëŠ” ê°ì†Œê°€ ê°œì„ 
                        }
            return result
        
        return compare_nested(current_metrics, baseline_metrics)
    
    def generate_optimization_report(self, metrics: Dict, scores: Dict, comparison: Optional[Dict] = None) -> str:
        """ìµœì í™” ë³´ê³ ì„œ ìƒì„±"""
        report = [
            "# ğŸš€ ì†ë„ ìµœì í™” ë¶„ì„ ë³´ê³ ì„œ",
            f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## ğŸ“Š í˜„ì¬ ì„±ëŠ¥ ì§€í‘œ",
        ]
        
        for category, category_metrics in metrics.items():
            report.append(f"\n### {category.title()} ì„±ëŠ¥")
            for metric, value in category_metrics.items():
                if isinstance(value, (int, float)):
                    report.append(f"- **{metric}**: {value:.3f}")
                elif isinstance(value, dict):
                    report.append(f"- **{metric}**:")
                    for sub_metric, sub_value in value.items():
                        if isinstance(sub_value, (int, float)):
                            report.append(f"  - {sub_metric}: {sub_value:.3f}")
        
        report.append("\n## ğŸ¯ ìµœì í™” ì ìˆ˜")
        for category, score in scores.items():
            status = "ğŸŸ¢" if score >= 80 else "ğŸŸ¡" if score >= 60 else "ğŸ”´"
            report.append(f"- **{category.title()}**: {score:.1f}ì  {status}")
        
        if comparison:
            report.append("\n## ğŸ“ˆ ê¸°ì¤€ì„  ëŒ€ë¹„ ê°œì„ ë„")
            # ë¹„êµ ê²°ê³¼ ì¶”ê°€ (ê°„ëµí™”)
            improved_count = 0
            total_count = 0
            
            def count_improvements(comp_dict):
                nonlocal improved_count, total_count
                for key, value in comp_dict.items():
                    if isinstance(value, dict):
                        if 'better' in value:
                            total_count += 1
                            if value['better']:
                                improved_count += 1
                        else:
                            count_improvements(value)
            
            count_improvements(comparison)
            if total_count > 0:
                improvement_rate = (improved_count / total_count) * 100
                report.append(f"- ì „ì²´ ì§€í‘œ ì¤‘ {improvement_rate:.1f}% ê°œì„ ")
        
        return "\n".join(report)


# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜ë“¤
def create_test_data():
    """í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±"""
    return {
        'test_texts': [
            "ê·¼ë¡œê³„ì•½ í•´ì§€ì— ê´€í•œ ë²•ì  ì ˆì°¨ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "ë¶€ë™ì‚° ì†Œìœ ê¶Œ ì´ì „ ë“±ê¸° ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            "êµí†µì‚¬ê³  ë°œìƒ ì‹œ ì†í•´ë°°ìƒ ì±…ì„ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.",
            "ì„ê¸ˆì²´ë¶ˆ ì‹œ ê·¼ë¡œìê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ë²•ì  ì¡°ì¹˜ëŠ”?",
            "ê³„ì•½ì„œ ì‘ì„± ì‹œ í•„ìˆ˜ì ìœ¼ë¡œ í¬í•¨í•´ì•¼ í•  ë‚´ìš©ì€?"
        ],
        'test_queries': [
            "ê·¼ë¡œê³„ì•½ í•´ì§€",
            "ë¶€ë™ì‚° ì†Œìœ ê¶Œ",
            "êµí†µì‚¬ê³  ì†í•´ë°°ìƒ",
            "ì„ê¸ˆì²´ë¶ˆ",
            "ê³„ì•½ì„œ ì‘ì„±"
        ]
    }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    metrics_system = SpeedOptimizationMetrics()
    
    # ëª©í‘œ ì§€í‘œ ì¶œë ¥
    targets = metrics_system.set_performance_targets()
    print("ğŸ¯ ìµœì í™” ëª©í‘œ ì§€í‘œ:")
    print(json.dumps(targets, indent=2, ensure_ascii=False))
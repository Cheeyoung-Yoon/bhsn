#!/usr/bin/env python3
"""
í’ˆì§ˆ í‰ê°€ê¸° ëª¨ë“ˆ

ì±—ë´‡ê³¼ RAG ì‹œìŠ¤í…œì˜ ì •ì„±ì  í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import time
from typing import Dict, List, Tuple, Optional
from datetime import datetime

# Add parent directories to path
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

try:
    from task2.app import LawChatbot
    from task1.app.parser import parse_cases
    from task1.app.config import DATA_JSON
except ImportError as e:
    print(f"Task 1/2 ëª¨ë“ˆ import ì˜¤ë¥˜: {e}")


class QualityEvaluator:
    """ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """í’ˆì§ˆ í‰ê°€ê¸° ì´ˆê¸°í™”"""
        self.chatbot = None
        self.test_questions = []
        self.evaluation_results = {}
        
    def initialize_chatbot(self):
        """ì±—ë´‡ ì´ˆê¸°í™”"""
        try:
            print("ğŸ¤– ì±—ë´‡ ì´ˆê¸°í™” ì¤‘...")
            self.chatbot = LawChatbot()
            print("   âœ… ì±—ë´‡ ì´ˆê¸°í™” ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"   âŒ ì±—ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def prepare_test_questions(self):
        """í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì¤€ë¹„"""
        self.test_questions = [
            {
                "category": "ê·¼ë¡œë²•",
                "question": "ì‚¬ì§ì›ì„ ì œì¶œí•œ í›„ ì² íšŒí•  ìˆ˜ ìˆë‚˜ìš”?",
                "expected_concepts": ["ì‚¬ì§", "ì² íšŒ", "ê·¼ë¡œê³„ì•½", "í•´ì§€"],
                "difficulty": "ë³´í†µ"
            },
            {
                "category": "ë¶€ë™ì‚°ë²•",
                "question": "ì†Œìœ ê¶Œë³´ì¡´ë“±ê¸°ì˜ ì¶”ì •ë ¥ì´ ê¹¨ì§€ëŠ” ê²½ìš°ëŠ” ì–¸ì œì¸ê°€ìš”?",
                "expected_concepts": ["ì†Œìœ ê¶Œë³´ì¡´ë“±ê¸°", "ì¶”ì •ë ¥", "ì‚¬ì •", "ì–‘ë„"],
                "difficulty": "ì–´ë ¤ì›€"
            },
            {
                "category": "ë¯¼ì‚¬ë²•",
                "question": "ê³„ì•½ì„œ ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                "expected_concepts": ["ê³„ì•½ì„œ", "ì‘ì„±", "ì£¼ì˜ì‚¬í•­"],
                "difficulty": "ì‰¬ì›€"
            },
            {
                "category": "êµí†µì‚¬ê³ ",
                "question": "êµí†µì‚¬ê³ ë¡œ ì‚¬ë§í•œ ì‚¬ì—…ìì˜ ì¼ì‹¤ì´ìµì€ ì–´ë–»ê²Œ ì‚°ì •í•˜ë‚˜ìš”?",
                "expected_concepts": ["êµí†µì‚¬ê³ ", "ì¼ì‹¤ì´ìµ", "ì‚¬ì—…ì", "ì‚°ì •"],
                "difficulty": "ë³´í†µ"
            },
            {
                "category": "ë…¸ë™ë²•",
                "question": "ëŒ€ê¸°ìš´ì „ì‚¬ë„ ì •ê·œì§ê³¼ ê°™ì€ ëŒ€ìš°ë¥¼ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?",
                "expected_concepts": ["ëŒ€ê¸°ìš´ì „ì‚¬", "ê·¼ë¬´", "ë°°ì°¨", "ë…¸ë™"],
                "difficulty": "ë³´í†µ"
            },
            {
                "category": "íŠ¹í—ˆë²•",
                "question": "ì‹¤ìš©ì‹ ì•ˆ ë“±ë¡ ìš”ê±´ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "expected_concepts": ["ì‹¤ìš©ì‹ ì•ˆ", "ë“±ë¡", "ìš”ê±´"],
                "difficulty": "ì–´ë ¤ì›€"
            }
        ]
        
        print(f"ğŸ“ {len(self.test_questions)}ê°œ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì¤€ë¹„ ì™„ë£Œ")
    
    def evaluate_response_quality(self, question: str, response: str, expected_concepts: List[str]) -> Dict:
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
        evaluation = {
            "relevance_score": 0,      # ê´€ë ¨ì„± ì ìˆ˜ (0-10)
            "completeness_score": 0,   # ì™„ì„±ë„ ì ìˆ˜ (0-10)
            "accuracy_score": 0,       # ì •í™•ë„ ì ìˆ˜ (0-10)
            "clarity_score": 0,        # ëª…í™•ì„± ì ìˆ˜ (0-10)
            "concept_coverage": 0,     # ê°œë… í¬í•¨ë„ (0-1)
            "has_legal_reference": False,  # ë²•ì  ê·¼ê±° í¬í•¨ ì—¬ë¶€
            "response_length": len(response),
            "issues": []
        }
        
        if not response or len(response) < 50:
            evaluation["issues"].append("ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ")
            return evaluation
        
        response_lower = response.lower()
        
        # 1. ê°œë… í¬í•¨ë„ í‰ê°€
        covered_concepts = 0
        for concept in expected_concepts:
            if concept.lower() in response_lower:
                covered_concepts += 1
        
        evaluation["concept_coverage"] = covered_concepts / len(expected_concepts)
        
        # 2. ë²•ì  ê·¼ê±° í¬í•¨ ì—¬ë¶€
        legal_indicators = ["íŒë¡€", "ë²•ì›", "ëŒ€ë²•ì›", "ì¡°", "í•­", "í˜¸", "ë²•ë¥ ", "íŒê²°"]
        evaluation["has_legal_reference"] = any(indicator in response for indicator in legal_indicators)
        
        # 3. ì‘ë‹µ í’ˆì§ˆ ì ìˆ˜ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        
        # ê´€ë ¨ì„±: ê¸°ëŒ€ ê°œë… í¬í•¨ë„ ê¸°ë°˜
        evaluation["relevance_score"] = min(10, evaluation["concept_coverage"] * 10 + 2)
        
        # ì™„ì„±ë„: ì‘ë‹µ ê¸¸ì´ì™€ êµ¬ì¡° ê¸°ë°˜
        if len(response) > 200:
            evaluation["completeness_score"] = 8
        elif len(response) > 100:
            evaluation["completeness_score"] = 6
        else:
            evaluation["completeness_score"] = 4
        
        # ì •í™•ë„: ë²•ì  ê·¼ê±° í¬í•¨ ì—¬ë¶€ ê¸°ë°˜
        if evaluation["has_legal_reference"]:
            evaluation["accuracy_score"] = 8
        elif evaluation["concept_coverage"] > 0.5:
            evaluation["accuracy_score"] = 6
        else:
            evaluation["accuracy_score"] = 4
        
        # ëª…í™•ì„±: êµ¬ì¡°í™”ëœ ì‘ë‹µì¸ì§€ í™•ì¸
        structure_indicators = ["ë‹µë³€:", "ì°¸ì¡°", "1.", "2.", "-", "â€¢"]
        has_structure = any(indicator in response for indicator in structure_indicators)
        evaluation["clarity_score"] = 8 if has_structure else 5
        
        # ë¬¸ì œì  ì²´í¬
        if evaluation["concept_coverage"] < 0.3:
            evaluation["issues"].append("ê´€ë ¨ ê°œë… ë¶€ì¡±")
        
        if not evaluation["has_legal_reference"]:
            evaluation["issues"].append("ë²•ì  ê·¼ê±° ë¶€ì¡±")
        
        if len(response) > 1000:
            evaluation["issues"].append("ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ìŒ")
        
        return evaluation
    
    def test_chatbot_responses(self) -> Dict:
        """ì±—ë´‡ ì‘ë‹µ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª ì±—ë´‡ ì‘ë‹µ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_results = {
            "total_questions": len(self.test_questions),
            "successful_responses": 0,
            "failed_responses": 0,
            "individual_results": [],
            "average_scores": {},
            "category_analysis": {}
        }
        
        if not self.chatbot:
            print("   âš ï¸ ì±—ë´‡ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return test_results
        
        all_scores = {
            "relevance": [],
            "completeness": [],
            "accuracy": [],
            "clarity": [],
            "concept_coverage": []
        }
        
        category_stats = {}
        
        for i, test_q in enumerate(self.test_questions, 1):
            question = test_q["question"]
            category = test_q["category"]
            expected_concepts = test_q["expected_concepts"]
            difficulty = test_q["difficulty"]
            
            print(f"   ğŸ“ ì§ˆë¬¸ {i}/{len(self.test_questions)}: {question[:50]}...")
            
            try:
                # ì±—ë´‡ì—ê²Œ ì§ˆë¬¸
                start_time = time.time()
                history = []
                _, updated_history = self.chatbot.chat(question, history)
                end_time = time.time()
                
                response_time = end_time - start_time
                
                if updated_history and len(updated_history) >= 2:
                    response = updated_history[-1]["content"]
                    
                    # ì‘ë‹µ í’ˆì§ˆ í‰ê°€
                    quality_eval = self.evaluate_response_quality(
                        question, response, expected_concepts
                    )
                    
                    # ê²°ê³¼ ì €ì¥
                    result = {
                        "question_id": i,
                        "question": question,
                        "category": category,
                        "difficulty": difficulty,
                        "response": response,
                        "response_time": response_time,
                        "quality_evaluation": quality_eval
                    }
                    
                    test_results["individual_results"].append(result)
                    test_results["successful_responses"] += 1
                    
                    # ì ìˆ˜ ëˆ„ì 
                    all_scores["relevance"].append(quality_eval["relevance_score"])
                    all_scores["completeness"].append(quality_eval["completeness_score"])
                    all_scores["accuracy"].append(quality_eval["accuracy_score"])
                    all_scores["clarity"].append(quality_eval["clarity_score"])
                    all_scores["concept_coverage"].append(quality_eval["concept_coverage"])
                    
                    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
                    if category not in category_stats:
                        category_stats[category] = {
                            "count": 0,
                            "avg_relevance": 0,
                            "avg_accuracy": 0,
                            "concept_coverage": 0
                        }
                    
                    category_stats[category]["count"] += 1
                    category_stats[category]["avg_relevance"] += quality_eval["relevance_score"]
                    category_stats[category]["avg_accuracy"] += quality_eval["accuracy_score"]
                    category_stats[category]["concept_coverage"] += quality_eval["concept_coverage"]
                    
                    print(f"      âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ ({response_time:.2f}ì´ˆ)")
                    print(f"         ê´€ë ¨ì„±: {quality_eval['relevance_score']}/10, "
                          f"ì •í™•ë„: {quality_eval['accuracy_score']}/10")
                    
                else:
                    print(f"      âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: ë¹ˆ ì‘ë‹µ")
                    test_results["failed_responses"] += 1
                    
            except Exception as e:
                print(f"      âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                test_results["failed_responses"] += 1
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        if all_scores["relevance"]:
            test_results["average_scores"] = {
                "relevance": sum(all_scores["relevance"]) / len(all_scores["relevance"]),
                "completeness": sum(all_scores["completeness"]) / len(all_scores["completeness"]),
                "accuracy": sum(all_scores["accuracy"]) / len(all_scores["accuracy"]),
                "clarity": sum(all_scores["clarity"]) / len(all_scores["clarity"]),
                "concept_coverage": sum(all_scores["concept_coverage"]) / len(all_scores["concept_coverage"])
            }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ì™„ë£Œ
        for category, stats in category_stats.items():
            if stats["count"] > 0:
                test_results["category_analysis"][category] = {
                    "question_count": stats["count"],
                    "avg_relevance": stats["avg_relevance"] / stats["count"],
                    "avg_accuracy": stats["avg_accuracy"] / stats["count"],
                    "avg_concept_coverage": stats["concept_coverage"] / stats["count"]
                }
        
        print(f"   âœ… ì±—ë´‡ ì‘ë‹µ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"      ì„±ê³µ: {test_results['successful_responses']}ê°œ, "
              f"ì‹¤íŒ¨: {test_results['failed_responses']}ê°œ")
        
        if test_results["average_scores"]:
            avg_scores = test_results["average_scores"]
            print(f"      í‰ê·  ì ìˆ˜ - ê´€ë ¨ì„±: {avg_scores['relevance']:.1f}/10, "
                  f"ì •í™•ë„: {avg_scores['accuracy']:.1f}/10")
        
        return test_results
    
    def analyze_response_patterns(self, test_results: Dict) -> Dict:
        """ì‘ë‹µ íŒ¨í„´ ë¶„ì„"""
        print("\nğŸ“Š ì‘ë‹µ íŒ¨í„´ ë¶„ì„ ì¤‘...")
        
        pattern_analysis = {
            "common_issues": {},
            "response_length_stats": {},
            "legal_reference_rate": 0,
            "difficulty_performance": {}
        }
        
        if not test_results["individual_results"]:
            return pattern_analysis
        
        # ê³µí†µ ë¬¸ì œì  ë¶„ì„
        all_issues = []
        response_lengths = []
        legal_ref_count = 0
        difficulty_scores = {}
        
        for result in test_results["individual_results"]:
            quality_eval = result["quality_evaluation"]
            
            # ë¬¸ì œì  ìˆ˜ì§‘
            all_issues.extend(quality_eval["issues"])
            
            # ì‘ë‹µ ê¸¸ì´ ìˆ˜ì§‘
            response_lengths.append(quality_eval["response_length"])
            
            # ë²•ì  ê·¼ê±° í¬í•¨ ì—¬ë¶€
            if quality_eval["has_legal_reference"]:
                legal_ref_count += 1
            
            # ë‚œì´ë„ë³„ ì„±ëŠ¥
            difficulty = result["difficulty"]
            if difficulty not in difficulty_scores:
                difficulty_scores[difficulty] = []
            difficulty_scores[difficulty].append(quality_eval["relevance_score"])
        
        # ê³µí†µ ë¬¸ì œì  ë¹ˆë„ ê³„ì‚°
        issue_counts = {}
        for issue in all_issues:
            issue_counts[issue] = issue_counts.get(issue, 0) + 1
        pattern_analysis["common_issues"] = issue_counts
        
        # ì‘ë‹µ ê¸¸ì´ í†µê³„
        if response_lengths:
            pattern_analysis["response_length_stats"] = {
                "average": sum(response_lengths) / len(response_lengths),
                "min": min(response_lengths),
                "max": max(response_lengths),
                "median": sorted(response_lengths)[len(response_lengths) // 2]
            }
        
        # ë²•ì  ê·¼ê±° í¬í•¨ë¥ 
        total_responses = len(test_results["individual_results"])
        pattern_analysis["legal_reference_rate"] = legal_ref_count / total_responses if total_responses > 0 else 0
        
        # ë‚œì´ë„ë³„ ì„±ëŠ¥
        for difficulty, scores in difficulty_scores.items():
            if scores:
                pattern_analysis["difficulty_performance"][difficulty] = {
                    "average_score": sum(scores) / len(scores),
                    "question_count": len(scores)
                }
        
        print(f"   âœ… ì‘ë‹µ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ")
        print(f"      ë²•ì  ê·¼ê±° í¬í•¨ë¥ : {pattern_analysis['legal_reference_rate']:.1%}")
        
        if pattern_analysis["common_issues"]:
            print("      ì£¼ìš” ë¬¸ì œì :")
            for issue, count in sorted(pattern_analysis["common_issues"].items(), 
                                     key=lambda x: x[1], reverse=True):
                print(f"         - {issue}: {count}íšŒ")
        
        return pattern_analysis
    
    def evaluate_system_quality(self) -> Dict:
        """ì‹œìŠ¤í…œ ì „ì²´ í’ˆì§ˆ í‰ê°€"""
        print("ğŸ¯ ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ ì‹œì‘")
        
        # ì±—ë´‡ ì´ˆê¸°í™”
        if not self.initialize_chatbot():
            return {"error": "ì±—ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨"}
        
        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì¤€ë¹„
        self.prepare_test_questions()
        
        # ì±—ë´‡ ì‘ë‹µ í…ŒìŠ¤íŠ¸
        test_results = self.test_chatbot_responses()
        
        # ì‘ë‹µ íŒ¨í„´ ë¶„ì„
        pattern_analysis = self.analyze_response_patterns(test_results)
        
        # ì¢…í•© í‰ê°€ ê²°ê³¼
        quality_evaluation = {
            "evaluation_timestamp": datetime.now().isoformat(),
            "test_results": test_results,
            "pattern_analysis": pattern_analysis,
            "overall_assessment": self.generate_overall_assessment(test_results, pattern_analysis)
        }
        
        print(f"\nâœ… ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ ì™„ë£Œ")
        
        return quality_evaluation
    
    def generate_overall_assessment(self, test_results: Dict, pattern_analysis: Dict) -> Dict:
        """ì „ì²´ì ì¸ í‰ê°€ ìƒì„±"""
        assessment = {
            "overall_grade": "ë¯¸í‰ê°€",
            "strengths": [],
            "weaknesses": [],
            "recommendations": []
        }
        
        if not test_results["average_scores"]:
            return assessment
        
        avg_scores = test_results["average_scores"]
        success_rate = test_results["successful_responses"] / test_results["total_questions"]
        
        # ì „ì²´ ë“±ê¸‰ ê³„ì‚°
        overall_score = (
            avg_scores["relevance"] * 0.3 +
            avg_scores["accuracy"] * 0.3 +
            avg_scores["completeness"] * 0.2 +
            avg_scores["clarity"] * 0.2
        )
        
        if overall_score >= 8.0:
            assessment["overall_grade"] = "ìš°ìˆ˜"
        elif overall_score >= 6.0:
            assessment["overall_grade"] = "ì–‘í˜¸"
        elif overall_score >= 4.0:
            assessment["overall_grade"] = "ë³´í†µ"
        else:
            assessment["overall_grade"] = "ê°œì„ í•„ìš”"
        
        # ê°•ì  ë¶„ì„
        if success_rate >= 0.9:
            assessment["strengths"].append("ë†’ì€ ì‘ë‹µ ì„±ê³µë¥ ")
        
        if avg_scores["relevance"] >= 7.0:
            assessment["strengths"].append("ì§ˆë¬¸ê³¼ ê´€ë ¨ì„± ë†’ì€ ì‘ë‹µ")
        
        if pattern_analysis["legal_reference_rate"] >= 0.5:
            assessment["strengths"].append("ë²•ì  ê·¼ê±° í¬í•¨ë¥  ì–‘í˜¸")
        
        # ì•½ì  ë¶„ì„
        if success_rate < 0.8:
            assessment["weaknesses"].append("ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ìœ¨ ë†’ìŒ")
        
        if avg_scores["accuracy"] < 6.0:
            assessment["weaknesses"].append("ë²•ì  ì •í™•ë„ ë¶€ì¡±")
        
        if pattern_analysis["legal_reference_rate"] < 0.3:
            assessment["weaknesses"].append("ë²•ì  ê·¼ê±° ë¶€ì¡±")
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        if avg_scores["concept_coverage"] < 0.5:
            assessment["recommendations"].append("ê´€ë ¨ ê°œë… í¬í•¨ë„ í–¥ìƒ í•„ìš”")
        
        if pattern_analysis["legal_reference_rate"] < 0.5:
            assessment["recommendations"].append("ë²•ì  ê·¼ê±° ì¸ìš© ê°•í™” í•„ìš”")
        
        if "ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ìŒ" in pattern_analysis["common_issues"]:
            assessment["recommendations"].append("ì‘ë‹µ ê¸¸ì´ ìµœì í™” í•„ìš”")
        
        return assessment


if __name__ == "__main__":
    evaluator = QualityEvaluator()
    results = evaluator.evaluate_system_quality()
    
    print("\nğŸ“Š í’ˆì§ˆ í‰ê°€ ê²°ê³¼:")
    print(json.dumps(results, indent=2, ensure_ascii=False))
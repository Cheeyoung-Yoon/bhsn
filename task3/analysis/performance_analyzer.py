#!/usr/bin/env python3
"""
ì„±ëŠ¥ ë¶„ì„ê¸° ëª¨ë“ˆ

ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ RAG ì‹œìŠ¤í…œì˜ ì •ëŸ‰ì  ì„±ëŠ¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
import json
import numpy as np
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import traceback

# Add parent directories to path
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

try:
    from task1.app.embedding_client import EmbeddingClient
    from task1.app.db_connection import VectorDB
    from task1.app.parser import parse_cases
    from task1.app.config import DATA_JSON
except ImportError as e:
    print(f"Task 1 ëª¨ë“ˆ import ì˜¤ë¥˜: {e}")


class PerformanceAnalyzer:
    """ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì„±ëŠ¥ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        self.embedder = None
        self.vector_db = None
        self.test_cases = []
        self.results = {}
        
    def initialize_components(self):
        """ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        try:
            print("ğŸ”§ ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘...")
            
            # ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            self.embedder = EmbeddingClient()
            print("   âœ… ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
            self.vector_db = VectorDB(dim=3072)  # Google Gemini embedding dimension
            print("   âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            print(f"   âŒ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def load_test_data(self):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        try:
            print("ğŸ“š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
            
            # ì¼€ì´ìŠ¤ ë°ì´í„° íŒŒì‹±
            records = parse_cases(DATA_JSON)
            self.test_cases = records[:10]  # ì²˜ìŒ 10ê°œ ì¼€ì´ìŠ¤ë§Œ ì‚¬ìš©
            
            print(f"   âœ… {len(self.test_cases)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"   âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def analyze_embedding_performance(self) -> Dict:
        """ì„ë² ë”© ì„±ëŠ¥ ë¶„ì„"""
        print("\nğŸ” ì„ë² ë”© ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
        
        embedding_results = {
            "average_time": 0,
            "throughput": 0,
            "dimension": 0,
            "batch_performance": {},
            "errors": 0
        }
        
        if not self.embedder:
            print("   âš ï¸ ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return embedding_results
        
        try:
            # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ì¤€ë¹„
            test_texts = []
            for case in self.test_cases:
                summary = case.get("íŒê²°ìš”ì§€", "")
                if summary and len(summary) > 50:
                    test_texts.append(summary[:500])  # ìµœëŒ€ 500ì
            
            if not test_texts:
                print("   âš ï¸ í…ŒìŠ¤íŠ¸í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŒ")
                return embedding_results
            
            # ê°œë³„ ì„ë² ë”© ì„±ëŠ¥ ì¸¡ì •
            times = []
            for i, text in enumerate(test_texts[:5]):  # ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
                start_time = time.time()
                try:
                    result = self.embedder.embed([text])
                    embedding_results["dimension"] = result["dim"]
                    end_time = time.time()
                    times.append(end_time - start_time)
                except Exception as e:
                    print(f"   âš ï¸ ì„ë² ë”© ì‹¤íŒ¨ {i+1}: {e}")
                    embedding_results["errors"] += 1
            
            if times:
                embedding_results["average_time"] = np.mean(times)
                embedding_results["throughput"] = 1.0 / np.mean(times)
            
            # ë°°ì¹˜ ì„±ëŠ¥ ì¸¡ì •
            batch_sizes = [1, 3, 5]
            for batch_size in batch_sizes:
                if len(test_texts) >= batch_size:
                    batch_texts = test_texts[:batch_size]
                    start_time = time.time()
                    try:
                        self.embedder.embed(batch_texts)
                        end_time = time.time()
                        batch_time = end_time - start_time
                        embedding_results["batch_performance"][batch_size] = {
                            "total_time": batch_time,
                            "time_per_item": batch_time / batch_size
                        }
                    except Exception as e:
                        print(f"   âš ï¸ ë°°ì¹˜ í¬ê¸° {batch_size} ì‹¤íŒ¨: {e}")
                        embedding_results["errors"] += 1
            
            print(f"   âœ… ì„ë² ë”© ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ")
            print(f"      - í‰ê·  ì‹œê°„: {embedding_results['average_time']:.3f}ì´ˆ")
            print(f"      - ì²˜ë¦¬ëŸ‰: {embedding_results['throughput']:.1f} texts/sec")
            print(f"      - ì°¨ì›: {embedding_results['dimension']}")
            
        except Exception as e:
            print(f"   âŒ ì„ë² ë”© ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            embedding_results["errors"] += 1
        
        return embedding_results
    
    def analyze_vector_db_performance(self) -> Dict:
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„"""
        print("\nğŸ—„ï¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„ ì¤‘...")
        
        db_results = {
            "search_performance": {},
            "index_stats": {},
            "errors": 0
        }
        
        if not self.vector_db or not self.embedder:
            print("   âš ï¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë˜ëŠ” ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return db_results
        
        try:
            # ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •
            test_queries = [
                "ê·¼ë¡œê³„ì•½ í•´ì§€",
                "ë¶€ë™ì‚° ì†Œìœ ê¶Œ",
                "êµí†µì‚¬ê³  ì†í•´ë°°ìƒ",
                "ì„ê¸ˆì²´ë¶ˆ",
                "ê³„ì•½ì„œ ì‘ì„±"
            ]
            
            search_times = []
            for query in test_queries:
                try:
                    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
                    query_embedding = self.embedder.embed_query(query)
                    
                    # ê²€ìƒ‰ ì‹œê°„ ì¸¡ì •
                    start_time = time.time()
                    results = self.vector_db.search(
                        query_vector=query_embedding.tolist(),
                        top_k=5
                    )
                    end_time = time.time()
                    
                    search_time = end_time - start_time
                    search_times.append(search_time)
                    
                    # ê²°ê³¼ í’ˆì§ˆ í™•ì¸
                    matches = results.get('matches', [])
                    print(f"   ğŸ“ ì¿¼ë¦¬ '{query}': {len(matches)}ê°œ ê²°ê³¼, {search_time:.3f}ì´ˆ")
                    
                except Exception as e:
                    print(f"   âš ï¸ ì¿¼ë¦¬ '{query}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    db_results["errors"] += 1
            
            if search_times:
                db_results["search_performance"] = {
                    "average_time": np.mean(search_times),
                    "min_time": np.min(search_times),
                    "max_time": np.max(search_times),
                    "std_time": np.std(search_times)
                }
            
            # ì¸ë±ìŠ¤ í†µê³„ (Pineconeì˜ ê²½ìš°)
            try:
                index_stats = self.vector_db.get_index_stats()
                db_results["index_stats"] = index_stats
            except Exception as e:
                print(f"   âš ï¸ ì¸ë±ìŠ¤ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                db_results["errors"] += 1
            
            print(f"   âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ")
            if search_times:
                avg_time = np.mean(search_times)
                print(f"      - í‰ê·  ê²€ìƒ‰ ì‹œê°„: {avg_time:.3f}ì´ˆ")
                print(f"      - ê²€ìƒ‰ ì²˜ë¦¬ëŸ‰: {1.0/avg_time:.1f} queries/sec")
            
        except Exception as e:
            print(f"   âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            db_results["errors"] += 1
        
        return db_results
    
    def analyze_retrieval_accuracy(self) -> Dict:
        """ê²€ìƒ‰ ì •í™•ë„ ë¶„ì„"""
        print("\nğŸ¯ ê²€ìƒ‰ ì •í™•ë„ ë¶„ì„ ì¤‘...")
        
        accuracy_results = {
            "precision_at_k": {},
            "recall_estimates": {},
            "relevance_scores": [],
            "errors": 0
        }
        
        if not self.vector_db or not self.embedder:
            print("   âš ï¸ í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            return accuracy_results
        
        try:
            # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬-ë‹µë³€ ìŒ
            test_pairs = [
                {
                    "query": "ê·¼ë¡œê³„ì•½ í•´ì§€ ì ˆì°¨",
                    "expected_keywords": ["ê·¼ë¡œê³„ì•½", "í•´ì§€", "í†µê³ ", "ì˜ì‚¬í‘œì‹œ"]
                },
                {
                    "query": "ë¶€ë™ì‚° ì†Œìœ ê¶Œ ë“±ê¸°",
                    "expected_keywords": ["ë¶€ë™ì‚°", "ì†Œìœ ê¶Œ", "ë“±ê¸°", "ì¶”ì •"]
                },
                {
                    "query": "êµí†µì‚¬ê³  ì†í•´ë°°ìƒ",
                    "expected_keywords": ["êµí†µì‚¬ê³ ", "ì†í•´ë°°ìƒ", "ê³¼ì‹¤", "ë°°ìƒ"]
                }
            ]
            
            k_values = [1, 3, 5]
            precision_scores = {k: [] for k in k_values}
            
            for test_pair in test_pairs:
                query = test_pair["query"]
                expected_keywords = test_pair["expected_keywords"]
                
                try:
                    # ì¿¼ë¦¬ ì„ë² ë”© ë° ê²€ìƒ‰
                    query_embedding = self.embedder.embed_query(query)
                    results = self.vector_db.search(
                        query_vector=query_embedding.tolist(),
                        top_k=max(k_values)
                    )
                    
                    matches = results.get('matches', [])
                    
                    # ê° kê°’ì— ëŒ€í•œ ì •í™•ë„ ê³„ì‚°
                    for k in k_values:
                        relevant_count = 0
                        top_k_matches = matches[:k]
                        
                        for match in top_k_matches:
                            metadata = match.get('metadata', {})
                            content = metadata.get('content', '').lower()
                            
                            # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ì„± íŒë‹¨
                            keyword_matches = sum(1 for kw in expected_keywords 
                                                if kw.lower() in content)
                            if keyword_matches >= len(expected_keywords) // 2:
                                relevant_count += 1
                        
                        precision = relevant_count / k if k > 0 else 0
                        precision_scores[k].append(precision)
                    
                    print(f"   ğŸ“ ì¿¼ë¦¬ '{query}': {len(matches)}ê°œ ê²°ê³¼")
                    
                except Exception as e:
                    print(f"   âš ï¸ ì¿¼ë¦¬ '{query}' ì •í™•ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
                    accuracy_results["errors"] += 1
            
            # í‰ê·  ì •í™•ë„ ê³„ì‚°
            for k in k_values:
                if precision_scores[k]:
                    accuracy_results["precision_at_k"][f"p@{k}"] = {
                        "mean": np.mean(precision_scores[k]),
                        "std": np.std(precision_scores[k]),
                        "scores": precision_scores[k]
                    }
            
            print(f"   âœ… ê²€ìƒ‰ ì •í™•ë„ ë¶„ì„ ì™„ë£Œ")
            for k in k_values:
                if f"p@{k}" in accuracy_results["precision_at_k"]:
                    mean_precision = accuracy_results["precision_at_k"][f"p@{k}"]["mean"]
                    print(f"      - Precision@{k}: {mean_precision:.3f}")
            
        except Exception as e:
            print(f"   âŒ ê²€ìƒ‰ ì •í™•ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            accuracy_results["errors"] += 1
        
        return accuracy_results
    
    def run_comprehensive_analysis(self) -> Dict:
        """ì¢…í•© ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ ì¢…í•© ì„±ëŠ¥ ë¶„ì„ ì‹œì‘")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        if not self.initialize_components():
            return {"error": "ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨"}
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        if not self.load_test_data():
            return {"error": "í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨"}
        
        # ê°ì¢… ë¶„ì„ ìˆ˜í–‰
        analysis_results = {
            "analysis_timestamp": datetime.now().isoformat(),
            "test_cases_count": len(self.test_cases),
            "embedding_performance": self.analyze_embedding_performance(),
            "vector_db_performance": self.analyze_vector_db_performance(),
            "retrieval_accuracy": self.analyze_retrieval_accuracy()
        }
        
        # ì „ì²´ ì˜¤ë¥˜ ìˆ˜ ê³„ì‚°
        total_errors = (
            analysis_results["embedding_performance"].get("errors", 0) +
            analysis_results["vector_db_performance"].get("errors", 0) +
            analysis_results["retrieval_accuracy"].get("errors", 0)
        )
        
        analysis_results["total_errors"] = total_errors
        analysis_results["analysis_status"] = "ì™„ë£Œ" if total_errors == 0 else "ê²½ê³ "
        
        print(f"\nâœ… ì¢…í•© ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ (ì˜¤ë¥˜: {total_errors}ê°œ)")
        
        return analysis_results


if __name__ == "__main__":
    analyzer = PerformanceAnalyzer()
    results = analyzer.run_comprehensive_analysis()
    
    print("\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
    print(json.dumps(results, indent=2, ensure_ascii=False))